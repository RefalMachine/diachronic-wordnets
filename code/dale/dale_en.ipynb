{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "def add_sys_path(p):\n",
    "    p = os.path.abspath(p)\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "add_sys_path('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import WordNetCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import lru_cache\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_taxonomy = WordNetCorpusReader('../../datasets/WNs/WN2.0', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3129, 2)\n"
     ]
    }
   ],
   "source": [
    "nouns_data = pd.read_csv('../../datasets/en/nouns_en.2.0-3.0.tsv', sep='\\t', header=None)\n",
    "nouns_data.columns = ['word', 'hypernyms']\n",
    "nouns_data['hypernyms'] = nouns_data['hypernyms'].apply(json.loads)\n",
    "print(nouns_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>hypernyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>Mount_Garmo</td>\n",
       "      <td>[peak.n.04, mountain_peak.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>open-hearth_furnace</td>\n",
       "      <td>[chamber.n.01, furnace.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>Hakka</td>\n",
       "      <td>[chinese.n.01, sinitic.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>fluid_flywheel</td>\n",
       "      <td>[flywheel.n.01, governor.n.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>Taklimakan_Desert</td>\n",
       "      <td>[biome.n.01, desert.n.01, tract.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>Congreve</td>\n",
       "      <td>[dramatist.n.01, writer.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>brachycephalic</td>\n",
       "      <td>[person.n.01, adult.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>24-karat_gold</td>\n",
       "      <td>[noble_metal.n.01, gold.n.03]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>Sealyham</td>\n",
       "      <td>[wirehair.n.01, welsh_terrier.n.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>prescriptivism</td>\n",
       "      <td>[belief.n.01, doctrine.n.01]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     word                              hypernyms\n",
       "1569          Mount_Garmo        [peak.n.04, mountain_peak.n.01]\n",
       "162   open-hearth_furnace           [chamber.n.01, furnace.n.01]\n",
       "1531                Hakka           [chinese.n.01, sinitic.n.01]\n",
       "771        fluid_flywheel         [flywheel.n.01, governor.n.02]\n",
       "2172    Taklimakan_Desert  [biome.n.01, desert.n.01, tract.n.01]\n",
       "783              Congreve          [dramatist.n.01, writer.n.01]\n",
       "1865       brachycephalic              [person.n.01, adult.n.01]\n",
       "1708        24-karat_gold          [noble_metal.n.01, gold.n.03]\n",
       "2322             Sealyham    [wirehair.n.01, welsh_terrier.n.01]\n",
       "759        prescriptivism           [belief.n.01, doctrine.n.01]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>hypernyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>overjoy</td>\n",
       "      <td>[gladden.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>whiteout</td>\n",
       "      <td>[cover.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>cha-cha</td>\n",
       "      <td>[move.v.03, dance.v.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>jackrabbit</td>\n",
       "      <td>[startle.v.02, move.v.03]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>play_tricks</td>\n",
       "      <td>[deceive.v.01, victimize.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>goose</td>\n",
       "      <td>[pinch.v.01, grip.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>slough_off</td>\n",
       "      <td>[remove.v.01, amputate.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>[interact.v.01, treat.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>metallize</td>\n",
       "      <td>[coat.v.01, cover.v.01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>desulfurize</td>\n",
       "      <td>[get_rid_of.v.01]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word                       hypernyms\n",
       "104      overjoy                  [gladden.v.01]\n",
       "153     whiteout                    [cover.v.01]\n",
       "83       cha-cha         [move.v.03, dance.v.02]\n",
       "197   jackrabbit       [startle.v.02, move.v.03]\n",
       "12   play_tricks  [deceive.v.01, victimize.v.01]\n",
       "20         goose         [pinch.v.01, grip.v.01]\n",
       "194   slough_off    [remove.v.01, amputate.v.01]\n",
       "0     do_well_by     [interact.v.01, treat.v.01]\n",
       "71     metallize         [coat.v.01, cover.v.01]\n",
       "103  desulfurize               [get_rid_of.v.01]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_data = pd.read_csv(os.path.join('../../datasets/en/verbs_en.2.0-3.0.tsv'), sep='\\t', header=None)\n",
    "verbs_data.columns = ['word', 'hypernyms']\n",
    "verbs_data['hypernyms'] = verbs_data['hypernyms'].apply(json.loads)\n",
    "print(verbs_data.shape)\n",
    "verbs_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate # from https://github.com/dialogue-evaluation/taxonomy-enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=50_000)\n",
    "def morph_parse(w):\n",
    "    return nltk.pos_tag([w])[0][1]\n",
    "\n",
    "tokenize = nltk.word_tokenize\n",
    "word2pos  = morph_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VB'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_parse(\"get\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air-ship']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('air-ship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://vectors.nlpl.eu/explore/embeddings/en/models/ gigaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_vecs = gensim.models.KeyedVectors.load_word2vec_format('../baselines/models/gigaword/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mispronounce_VERB', 0.4861705005168915),\n",
       " ('type_VERB', 0.4858720600605011),\n",
       " ('typing_NOUN', 0.4777812957763672),\n",
       " ('bookmark_VERB', 0.47726932168006897),\n",
       " ('spell-check_NOUN', 0.4647689461708069),\n",
       " ('facebook_NOUN', 0.46326524019241333),\n",
       " ('retype_VERB', 0.4609655737876892),\n",
       " ('username_ADJ', 0.45739173889160156),\n",
       " ('mistype_VERB', 0.452288419008255),\n",
       " ('login_NOUN', 0.4498198628425598)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw_vecs.most_similar('google_VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 13.1% 217.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 15.0% 249.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 17.8% 295.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 44.9% 746.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================---------------------------] 46.8% 778.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================------------] 77.7% 1292.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================-----------] 79.6% 1323.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 85.8% 1426.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================================-------] 87.9% 1460.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 98.7% 1640.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "google_vecs = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=50_000)\n",
    "def lemmatize(word):\n",
    "    return lemmatizer.lemmatize(word) or word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v, epsilon=1e-10):\n",
    "    return v / (sum(v**2)**0.5 + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSentenceEmbedder:\n",
    "    def __init__(self, n=300, normalize_word=True, pos_weights=None, default_weight=1):\n",
    "        self.n = n\n",
    "        self.normalize_word = normalize_word\n",
    "        self.pos_weights = pos_weights\n",
    "        self.default_weight = default_weight\n",
    "\n",
    "    def get_word_vec(self, word):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @lru_cache(maxsize=1024)\n",
    "    def __call__(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        weights = self.get_word_weights(tokens)\n",
    "        vecs = [self.get_word_vec(word) for word in tokens]\n",
    "        if self.normalize_word:\n",
    "            vecs = [normalize(v) for v in vecs]\n",
    "        if len(vecs) == 0:\n",
    "            return np.zeros(self.n)\n",
    "        return normalize(sum([vec * weight for vec, weight in zip(vecs, weights)]))\n",
    "\n",
    "    def get_word_weights(self, tokens):\n",
    "        if not self.pos_weights:\n",
    "            return [1] * len(tokens)\n",
    "        return [self.pos_weights.get(word2pos(t), self.default_weight) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VWrapper(BaseSentenceEmbedder):\n",
    "    POS_MAP = {\n",
    "        'INFN': 'VERB',\n",
    "        'ADJF': 'ADJ',\n",
    "        'ADVB': 'ADV',\n",
    "    }\n",
    "    POS_MISS = {\n",
    "        'PREP'\n",
    "    }\n",
    "\n",
    "    def __init__(self, w2v, morph=morph_parse, append_pos=False, **kwargs):\n",
    "        super(W2VWrapper, self).__init__(**kwargs)\n",
    "        self.w2v = w2v\n",
    "        self.morph = morph\n",
    "        self.prefix2word = self.make_prefixes()\n",
    "        self.append_pos = append_pos\n",
    "\n",
    "    def make_prefixes(self):\n",
    "        prefix2word = defaultdict(set)\n",
    "        for w in tqdm(self.w2v.vocab.keys()):\n",
    "            for n in range(1, len(w) - 2):\n",
    "                prefix2word[w[:-n]].add(w)\n",
    "        prefix2word = {k: v for k, v in prefix2word.items()}\n",
    "        return prefix2word\n",
    "\n",
    "    def find_prefix(self, word, min_len=2):\n",
    "        mapping = self.prefix2word\n",
    "        if word in mapping:\n",
    "            return mapping[word]\n",
    "        for i in range(1, len(word) - min_len):\n",
    "            t = mapping.get(word[:-i])\n",
    "            if t is not None:\n",
    "                return t\n",
    "        return None\n",
    "\n",
    "    def get_text_vec(self, text, verbose=False):\n",
    "        toks = tokenize(text)\n",
    "        vecs = []\n",
    "        for tok in toks:\n",
    "            vecs.append(self.get_word_vec(word=tok, verbose=verbose))\n",
    "        if not vecs:\n",
    "            return np.zeros(self.w2v.vectors.shape[1])\n",
    "        return normalize(sum(vecs))\n",
    "\n",
    "    def add_pos(self, word):\n",
    "        tag = self.morph(word)\n",
    "        if not tag:\n",
    "            return word\n",
    "        new_pos = self.POS_MAP.get(tag, tag or '-')\n",
    "        return word + '_' + new_pos\n",
    "    \n",
    "    def get_word_vec(self, word, verbose=False, add_pos=False):\n",
    "        key = word\n",
    "        key2 = lemmatize(word)\n",
    "        if self.append_pos:\n",
    "            key = self.add_pos(key)\n",
    "            key2 = self.add_pos(key2)\n",
    "        if verbose:\n",
    "            print(key, key2, self.find_prefix(key))\n",
    "        if key in self.w2v.vocab:\n",
    "            return self.w2v[key]\n",
    "        elif key2 in self.w2v.vocab:\n",
    "            return self.w2v[key2]\n",
    "        else:\n",
    "            keys = self.find_prefix(key)\n",
    "            if keys:\n",
    "                return sum([self.w2v[k] for k in keys]) / len(keys)\n",
    "        return np.zeros(self.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82402d12157a441c9e0c2fd4b8d0aef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=297790), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder_gw = W2VWrapper(gw_vecs, append_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_MAP = {\n",
    "    #CC coordinating conjunction\n",
    "    'CD': 'NUM', # cardinal digit\n",
    "    #DT determiner\n",
    "    #EX existential there (like: “there is” … think of it like “there exists”)\n",
    "    'FW': 'X', # foreign word\n",
    "    #IN preposition/subordinating conjunction\n",
    "    'JJ': 'ADJ', #adjective ‘big’\n",
    "    'JJR': 'ADJ', #adjective, comparative ‘bigger’\n",
    "    'JJS': 'ADJ', #adjective, superlative ‘biggest’\n",
    "    'LS': 'NUM', #list marker 1)\n",
    "    'MD': 'VERB',# modal could, will\n",
    "    'NN': 'NOUN', #noun, singular ‘desk’\n",
    "    'NNS': 'NOUN', #noun plural ‘desks’\n",
    "    'NNP': 'PROPN', #proper noun, singular ‘Harrison’\n",
    "    'NNPS': 'PROPN', #proper noun, plural ‘Americans’\n",
    "    #PDT predeterminer ‘all the kids’\n",
    "    'POS': 'NOUN', #possessive ending parent’s\n",
    "    #'PRP personal pronoun I, he, she\n",
    "    # PRP$ possessive pronoun my, his, hers\n",
    "    'RB': 'ADV', #adverb very, silently,\n",
    "    'RBR': 'ADV', #adverb, comparative better\n",
    "    'RBS': 'ADV', #adverb, superlative best\n",
    "    #RP particle give up\n",
    "    #TO, to go ‘to’ the store.\n",
    "    #UH interjection, errrrrrrrm\n",
    "    'VB': 'VERB', #verb, base form take\n",
    "    'VBD': 'VERB',# verb, past tense, took\n",
    "    'VBG': 'VERB', #verb, gerund/present participle taking\n",
    "    'VBN': 'VERB', #verb, past participle is taken\n",
    "    'VBP': 'VERB', #verb, sing. present, known-3d take\n",
    "    'VBZ': 'VERB', #verb, 3rd person sing. present takes\n",
    "    #WDT wh-determiner which\n",
    "    #WP wh-pronoun who, what\n",
    "    #WP$ possessive wh-pronoun whose\n",
    "    #WRB wh-adverb where, when\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embedder_gw.POS_MAP = POS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdfac7975d24db2a1de8eea7a173833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=297790), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7924ff05e14da19da989a7a844f102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=297790), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder_gw_pos_n = W2VWrapper(gw_vecs,  pos_weights={'NN': 1.0, 'NNS': 1, 'NNP': 1, 'NNPS':1, 'IN': 0.1, 'CC': 0.1}, default_weight=0.5)\n",
    "w2v_embedder_gw_pos_v = W2VWrapper(gw_vecs,  pos_weights={'VB': 1.0, 'VBG': 1, 'IN': 0.1, 'CC': 0.1} , default_weight=0.5)\n",
    "w2v_embedder_gw_pos_n.POS_MAP = POS_MAP\n",
    "w2v_embedder_gw_pos_v.POS_MAP = POS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40b3d957b79476ebf3e640ed67145e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder = W2VWrapper(google_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b6ec99b1b34428817e06df69adf75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder_pos_n = W2VWrapper(google_vecs,  pos_weights={'NN': 1.0, 'IN': 0.1, 'CC': 0.1}, default_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974455761f7c4dfb905d21b174ff399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_embedder_pos_v = W2VWrapper(google_vecs,  pos_weights={'VB': 1.0, 'IN': 0.1, 'CC': 0.1}, default_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynsetStorage:\n",
    "    def __init__(self, id2synset, ids, ids_long, texts_long, word2sense, forbidden_id=None):\n",
    "        self.id2synset = id2synset\n",
    "        self.ids = ids\n",
    "        self.ids_long = ids_long\n",
    "        self.texts_long = texts_long\n",
    "        self.word2sense = word2sense\n",
    "        self.forbidden_id = forbidden_id or set()\n",
    "\n",
    " \n",
    "    @classmethod\n",
    "    def from_taxonomy(cls, tx, pos, forbidden_words=None):\n",
    "        id2synset = {v.name(): v for v in tx.all_synsets(pos=pos)}\n",
    "        print('synsets: {}'.format(len(id2synset)))\n",
    "        \n",
    "        word2sense = defaultdict(set)\n",
    "        for synset_id, synset in id2synset.items():\n",
    "            texts = {l.name() for l in synset.lemmas()}\n",
    "            texts.add(synset.name().split('.')[0])\n",
    "            # texts.add(synset.definition())\n",
    "            for text in texts:\n",
    "                word2sense[text].add(synset_id)\n",
    "        print('senses', len(word2sense))\n",
    "        \n",
    "        forbidden_id = set()\n",
    "        for word in (forbidden_words or {}):\n",
    "            if word in word2sense:\n",
    "                for sense_id in word2sense[word]:\n",
    "                    forbidden_id.add(sense_id)\n",
    "        print('forbidden senses are', len(forbidden_id))\n",
    "        \n",
    "        ids = sorted(id2synset.keys())\n",
    "        ids_long = []\n",
    "        texts_long = []\n",
    "        for id in tqdm(ids):\n",
    "            synset = id2synset[id]\n",
    "\n",
    "            texts = {l.name() for l in synset.lemmas()}\n",
    "            texts.add(synset.name().split('.')[0])\n",
    "            texts.add(synset.definition())\n",
    "\n",
    "            # исключаем все слова, омонимичные с тем, что есть в тестовой выборке\n",
    "            senses = {synset_id for w in texts for synset_id in word2sense[w]}\n",
    "            if senses.intersection(forbidden_id):\n",
    "                continue\n",
    "\n",
    "            if len(texts) > 1:\n",
    "                texts.add(' ; '.join(sorted(texts)))\n",
    "            for text in sorted(texts):\n",
    "                ids_long.append(id)\n",
    "                texts_long.append(text)\n",
    "        \n",
    "        print('numer of ids', len(ids), 'long list is', len(ids_long))\n",
    "        return cls(\n",
    "            id2synset=id2synset,\n",
    "            ids=ids,\n",
    "            ids_long=ids_long,\n",
    "            texts_long=texts_long,\n",
    "            word2sense=word2sense,\n",
    "            forbidden_id=forbidden_id,\n",
    "        )\n",
    "\n",
    "    def get_synset_name(self, synset_id):\n",
    "        return self.id2synset.get(synset_id, {}).get('@ruthes_name', '-')\n",
    "\n",
    "\n",
    "def make_rel_df(rel_n_raw, id2synset):\n",
    "    rel_df = pd.DataFrame(rel_n_raw['relations']['relation'])\n",
    "    rel_df['parent'] = rel_df['@parent_id'].apply(lambda x: id2synset[x]['@ruthes_name'])\n",
    "    rel_df['child'] = rel_df['@child_id'].apply(lambda x: id2synset.get(x, {}).get('@ruthes_name'))\n",
    "    return rel_df\n",
    "\n",
    "\n",
    "class RelationStorage:\n",
    "    def __init__(self, forbidden_id=None):\n",
    "        self.id2hyponym = defaultdict(set)\n",
    "        self.id2hypernym = defaultdict(set)\n",
    "        self.forbidden_id = forbidden_id or set()  # forbidden_id = set(ttest.SYNSET_ID)\n",
    "\n",
    "    def add_pair(self, hypo_id, hyper_id, max_depth=100500):\n",
    "        if max_depth <= 0:\n",
    "            return\n",
    "        if hypo_id in self.id2hyponym[hyper_id]:\n",
    "            # the pair is already here\n",
    "            return\n",
    "        if hypo_id in self.id2hypernym[hyper_id]:\n",
    "            raise ValueError('{} is already a hypernym of {}, so it cannot become its hyponym'.format(hypo_id, hyper_id))\n",
    "        for next_hypo in self.id2hyponym[hypo_id]:\n",
    "            self.add_pair(next_hypo, hyper_id, max_depth=max_depth-1)\n",
    "        for next_hyper in self.id2hypernym[hyper_id]:\n",
    "            self.add_pair(hypo_id, next_hyper, max_depth=max_depth-1)\n",
    "        self.id2hyponym[hyper_id].add(hypo_id)\n",
    "        self.id2hypernym[hypo_id].add(hyper_id)\n",
    "\n",
    "    def load_relations_from_taxonomy(self, tx, pos):\n",
    "        self.id2hyponym = defaultdict(set)\n",
    "        self.id2hypernym = defaultdict(set)\n",
    "        for v in tx.all_synsets(pos=pos):\n",
    "            hypo_id = v.name()\n",
    "            if hypo_id in self.forbidden_id:\n",
    "                continue\n",
    "            for hyper in v.hypernyms():\n",
    "                hyper_id = hyper.name()\n",
    "                if hyper_id not in self.forbidden_id:\n",
    "                    self.add_pair(hypo_id, hyper_id, max_depth=1)\n",
    "        print(len(self.id2hyponym))\n",
    "        print(max(len(c) for c in self.id2hyponym.values()))\n",
    "        print(max(len(c) for c in self.id2hypernym.values()))\n",
    "        print(sum(len(c) for c in self.id2hypernym.values()))\n",
    "            \n",
    "        \n",
    "    def construct_relations(self, rel_df):\n",
    "        self.id2hyponym = defaultdict(set)\n",
    "        self.id2hypernym = defaultdict(set)\n",
    "\n",
    "        hypo_df = rel_df[rel_df['@name'] == 'hyponym']\n",
    "        for r, row in tqdm(hypo_df.iterrows()):\n",
    "            hypo_id = row['@child_id']\n",
    "            hyper_id = row['@parent_id']\n",
    "            if hypo_id not in self.forbidden_id and hyper_id not in self.forbidden_id:\n",
    "                self.add_pair(hypo_id, hyper_id, max_depth=1)  # во второй версии поставим максимальную глубину, равную 2\n",
    "\n",
    "        print(len(self.id2hyponym))\n",
    "        print(max(len(c) for c in self.id2hyponym.values()))\n",
    "        print(max(len(c) for c in self.id2hypernym.values()))\n",
    "        print(sum(len(c) for c in self.id2hypernym.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypotheses_knn(\n",
    "        text,\n",
    "        synset_storage: SynsetStorage,\n",
    "        rel_storage: RelationStorage,\n",
    "        index=None,\n",
    "        text2vec=None,\n",
    "        k=10,\n",
    "        verbose=False,\n",
    "        decay=0,\n",
    "        grand_mult=1,\n",
    "        result_size=10,\n",
    "        return_hypotheses=False,\n",
    "        neighbor_scorer=None,\n",
    "        indexer=None,\n",
    "        scorer_pow=None,\n",
    "):\n",
    "    ids_list = synset_storage.ids_long\n",
    "    texts_list = synset_storage.texts_long\n",
    "    # todo: distance decay\n",
    "    if indexer:\n",
    "        distances, indices = indexer.query(text, k=k)\n",
    "    else:\n",
    "        vec = text2vec(text)\n",
    "        distances, indices = index.query(vec.reshape(1, -1), k=k)\n",
    "    hypotheses = Counter()\n",
    "    for i, d in zip(indices.ravel(), distances.ravel()):\n",
    "        s = 1 - d**2 / 2\n",
    "        hypers = rel_storage.id2hypernym.get(ids_list[i], set())\n",
    "\n",
    "        if neighbor_scorer is not None:\n",
    "            neighbor_score = neighbor_scorer(text, texts_list[i])\n",
    "        elif scorer_pow is not None:\n",
    "            neighbor_score = s**scorer_pow\n",
    "        else:\n",
    "            neighbor_score = 1\n",
    "        base_score = np.exp(-d ** decay) * neighbor_score\n",
    "        if verbose:\n",
    "            print(d, 1, ids_list[i], texts_list[i], len(hypers), np.exp(-d ** decay),  base_score)\n",
    "        for parent in hypers:\n",
    "            hypotheses[parent] += base_score\n",
    "            for grandparent in rel_storage.id2hypernym.get(parent, set()):\n",
    "                hypotheses[grandparent] += base_score * grand_mult\n",
    "    if return_hypotheses:\n",
    "        return hypotheses\n",
    "    if verbose:\n",
    "        print(len(hypotheses))\n",
    "    result = []\n",
    "    for hypo, cnt in hypotheses.most_common(result_size):\n",
    "        if verbose:\n",
    "            print(cnt, hypo, synset_storage.id2synset[hypo].name())\n",
    "        result.append(hypo)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets: 79689\n",
      "senses 132127\n",
      "forbidden senses are 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd3c2247e224325b9f891ac633c93d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=79689), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numer of ids 79689 long list is 319887\n"
     ]
    }
   ],
   "source": [
    "full_syn_storage = SynsetStorage.from_taxonomy(tx=existing_taxonomy, pos=existing_taxonomy.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79689\n",
      "619\n",
      "6\n",
      "81857\n"
     ]
    }
   ],
   "source": [
    "rel_storage = RelationStorage(forbidden_id=full_syn_storage.forbidden_id)\n",
    "rel_storage.load_relations_from_taxonomy(tx=existing_taxonomy, pos=existing_taxonomy.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedder is the longest part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa764145a05f44c899244ec4d8e5e5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=319887), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs = np.stack([w2v_embedder(t) for t in tqdm(full_syn_storage.texts_long) ])\n",
    "full_w2v_tree = KDTree(full_w2v_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f2d7332ad640fc884885c25765a165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=319887), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_pos = np.stack([w2v_embedder_pos_n(t) for t in tqdm(full_syn_storage.texts_long) ])\n",
    "full_w2v_tree_pos = KDTree(full_w2v_vecs_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input_text(text):\n",
    "    return text.replace('_', ' ').replace('-', ' ').lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_scorer(text1, text2):\n",
    "    return np.dot(w2v_embedder.get_text_vec(text1), w2v_embedder.get_text_vec(text2)) ** 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2submission(word2hypotheses, id2synset):\n",
    "    result_nouns = []\n",
    "    result_hyperonyms = []\n",
    "    for n, h in word2hypotheses.items():\n",
    "        for hypo in h:\n",
    "            result_nouns.append(n)\n",
    "            result_hyperonyms.append(hypo)\n",
    "    result_df = pd.DataFrame({'noun': result_nouns, 'result': result_hyperonyms})\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit for nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876b183b284144a5bf597e5c96f65760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_n_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree, text2vec=w2v_embedder_pos_n,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.8,\n",
    "        scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(nouns_data.word)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>light.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>actinic_radiation.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>fish_family.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>fish_genus.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>compound.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>pigment.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>family.n.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>cathode.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>genus.n.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phosphor</td>\n",
       "      <td>mineral.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>homozygote</td>\n",
       "      <td>asterid_dicot_genus.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>homozygote</td>\n",
       "      <td>animal_order.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>homozygote</td>\n",
       "      <td>dicot_genus.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>homozygote</td>\n",
       "      <td>receptor.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>homozygote</td>\n",
       "      <td>blocker.n.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          noun                    result\n",
       "0     phosphor                light.n.01\n",
       "1     phosphor    actinic_radiation.n.01\n",
       "2     phosphor          fish_family.n.01\n",
       "3     phosphor           fish_genus.n.01\n",
       "4     phosphor             compound.n.01\n",
       "5     phosphor              pigment.n.01\n",
       "6     phosphor               family.n.06\n",
       "7     phosphor              cathode.n.01\n",
       "8     phosphor                genus.n.02\n",
       "9     phosphor              mineral.n.01\n",
       "10  homozygote  asterid_dicot_genus.n.01\n",
       "11  homozygote         animal_order.n.01\n",
       "12  homozygote          dicot_genus.n.01\n",
       "13  homozygote             receptor.n.01\n",
       "14  homozygote              blocker.n.02"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = dict2submission(test_n_hypos, full_syn_storage.id2synset)\n",
    "sub.to_csv('../baselines/predictions/nouns_test_result_en.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "sub.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74de188a42ab4f33a602ee4d5852025f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_n_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_pos, text2vec=w2v_embedder_pos_n,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer,\n",
    "        #scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(nouns_data.word)\n",
    "}\n",
    "sub = dict2submission(test_n_hypos, full_syn_storage.id2synset)\n",
    "sub.to_csv('../baselines/predictions/nouns_en_2.0-3.0_test_result_dale.tsv', sep='\\t', encoding='utf-8', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: 0.24928523971081248\n",
      "mrr: 0.27100554343874983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python evaluate.py ../../datasets/en/nouns_en.2.0-3.0.tsv ../baselines/predictions/nouns_test_result_dale.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2838681f264c559433d0d70a371813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=319887), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_n_GW = np.stack([w2v_embedder_gw(t) for t in tqdm(full_syn_storage.texts_long) ])\n",
    "full_w2v_tree_n_GW = KDTree(full_w2v_vecs_n_GW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c26111c5db24bceb145e4b6e4daba98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=319887), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_gw_pos_n = np.stack([w2v_embedder_gw_pos_n(t) for t in tqdm(full_syn_storage.texts_long) ])\n",
    "full_w2v_tree_gw_pos_n = KDTree(full_w2v_vecs_gw_pos_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895907ff0c6949afb5ecaa7dc7b7d110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "map: 0.24928523971081248\n",
      "mrr: 0.27100554343874983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def w2v_scorer(text1, text2):\n",
    "    return np.dot(w2v_embedder_gw.get_text_vec(text1), w2v_embedder_gw.get_text_vec(text2)) ** 5\n",
    "\n",
    "test_v_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_n_GW, text2vec=w2v_embedder_gw,\n",
    "        #index=full_w2v_tree_gw_pos_v, text2vec=w2v_embedder_gw_pos_v,\n",
    "        synset_storage=full_syn_storage, rel_storage=rel_storage,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer,\n",
    "        #scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(nouns_data.word)\n",
    "}\n",
    "sub = dict2submission(test_n_hypos, full_syn_storage.id2synset)\n",
    "sub.to_csv('../baselines/predictions/nouns_en_2.0-3.0_test_result_dale_upd.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "! python evaluate.py ../../datasets/en/nouns_en.2.0-3.0.tsv ../baselines/predictions/nouns_en_2.0-3.0_test_result_dale_upd.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets: 13508\n",
      "senses 11319\n",
      "forbidden senses are 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e515bc6a1040cc95787be7ff4888f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13508), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numer of ids 13508 long list is 51663\n",
      "13285\n",
      "393\n",
      "2\n",
      "12985\n"
     ]
    }
   ],
   "source": [
    "full_syn_storage_v = SynsetStorage.from_taxonomy(tx=existing_taxonomy, pos=existing_taxonomy.VERB)\n",
    "rel_storage_v = RelationStorage(forbidden_id=full_syn_storage_v.forbidden_id)\n",
    "rel_storage_v.load_relations_from_taxonomy(tx=existing_taxonomy, pos=existing_taxonomy.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8b0017cc484f0fa5630ceb2aa459eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51663), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_v = np.stack([w2v_embedder(t) for t in tqdm(full_syn_storage_v.texts_long) ])\n",
    "full_w2v_tree_v = KDTree(full_w2v_vecs_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b409d5dbde2f42469f0ccd187439481f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51663), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_v_pos = np.stack([w2v_embedder_pos_v(t) for t in tqdm(full_syn_storage_v.texts_long) ])\n",
    "full_w2v_tree_v_pos = KDTree(full_w2v_vecs_v_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776190050bf046e7bae2182b4098f2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51663), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_v_GW = np.stack([w2v_embedder_gw(t) for t in tqdm(full_syn_storage_v.texts_long) ])\n",
    "full_w2v_tree_v_GW = KDTree(full_w2v_vecs_v_GW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab4fc4efca34a3e90f45010eb61712e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51663), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_w2v_vecs_gw_pos_v = np.stack([w2v_embedder_gw_pos_v(t) for t in tqdm(full_syn_storage_v.texts_long) ])\n",
    "full_w2v_tree_gw_pos_v = KDTree(full_w2v_vecs_gw_pos_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176f05eba82b481e96bda579fdd779b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=207), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_v_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_v, text2vec=w2v_embedder,\n",
    "        synset_storage=full_syn_storage_v, rel_storage=rel_storage_v,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(verbs_data.word)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>move.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>draw.v.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>travel.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>judge.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>remove.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>fudge.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>change.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>make.v.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>act.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>do_well_by</td>\n",
       "      <td>change.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>orb</td>\n",
       "      <td>be.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>orb</td>\n",
       "      <td>change.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>orb</td>\n",
       "      <td>idle.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>orb</td>\n",
       "      <td>interact.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>orb</td>\n",
       "      <td>form.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>orb</td>\n",
       "      <td>change_surface.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>orb</td>\n",
       "      <td>make.v.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>orb</td>\n",
       "      <td>interpret.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>orb</td>\n",
       "      <td>change.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>orb</td>\n",
       "      <td>meet.v.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>transition</td>\n",
       "      <td>change.v.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>transition</td>\n",
       "      <td>make.v.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>transition</td>\n",
       "      <td>grow_up.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>transition</td>\n",
       "      <td>weaken.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>transition</td>\n",
       "      <td>treat.v.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          noun               result\n",
       "0   do_well_by            move.v.02\n",
       "1   do_well_by            draw.v.07\n",
       "2   do_well_by          travel.v.01\n",
       "3   do_well_by           judge.v.02\n",
       "4   do_well_by          remove.v.01\n",
       "5   do_well_by           fudge.v.01\n",
       "6   do_well_by          change.v.02\n",
       "7   do_well_by            make.v.42\n",
       "8   do_well_by             act.v.01\n",
       "9   do_well_by          change.v.01\n",
       "10         orb              be.v.01\n",
       "11         orb          change.v.02\n",
       "12         orb            idle.v.02\n",
       "13         orb        interact.v.01\n",
       "14         orb            form.v.01\n",
       "15         orb  change_surface.v.01\n",
       "16         orb            make.v.03\n",
       "17         orb       interpret.v.01\n",
       "18         orb          change.v.01\n",
       "19         orb            meet.v.08\n",
       "20  transition          change.v.02\n",
       "21  transition            make.v.03\n",
       "22  transition         grow_up.v.01\n",
       "23  transition          weaken.v.01\n",
       "24  transition           treat.v.01"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = dict2submission(test_v_hypos, full_syn_storage_v.id2synset)\n",
    "sub.to_csv('../baselines/predictions/verbs_en_2.0-3.0_test_result_dale.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "sub.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map: 0.23318735093346502\n",
      "mrr: 0.236641582366971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python evaluate.py ../../datasets/en/verbs_en.2.0-3.0.tsv ../baselines/predictions/verbs_en_2.0-3.0_test_result_dale.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad684e78f40476e9cd50efbfb92ab19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=207), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "map: 0.24767456205280045\n",
      "mrr: 0.25423760177646193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_v_hypos = {\n",
    "    txt: \n",
    "    hypotheses_knn(\n",
    "        normalize_input_text(txt), \n",
    "        #index=full_ft_tree, text2vec=ft_embedder, \n",
    "        index=full_w2v_tree_v_pos, text2vec=w2v_embedder_pos_v,\n",
    "        synset_storage=full_syn_storage_v, rel_storage=rel_storage_v,\n",
    "        decay=3, \n",
    "        k=100, \n",
    "        grand_mult=0.5,\n",
    "        neighbor_scorer=w2v_scorer,\n",
    "        #scorer_pow=5\n",
    "    )   \n",
    "    for txt in tqdm(verbs_data.word)\n",
    "}\n",
    "sub = dict2submission(test_v_hypos, full_syn_storage_v.id2synset)\n",
    "sub.to_csv('../baselines/predictions/verbs_en_2.0-3.0_test_result_dale_upd.tsv', sep='\\t', encoding='utf-8', header=None, index=None)\n",
    "! python evaluate.py ../../datasets/en/verbs_en.2.0-3.0.tsv ../baselines/predictions/verbs_en_2.0-3.0_test_result_dale_upd.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
