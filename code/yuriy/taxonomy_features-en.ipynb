{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../taxonomy-enrichment/baselines/ruwordnet')\n",
    "sys.path.append('../../taxonomy-enrichment/baselines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "from nltk.corpus import WordNetCorpusReader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet = WordNetCorpusReader(\"D:\\dialogue2020\\semevals\\semeval-2016-task-14\\WN2.0\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "with open('D:/dialogue2020/semevals/semeval-2016-task-14/reader/no_labels_nouns_en_new.2.0-3.0.tsv', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ready-made', 'rancidness', 'heterometaboly'], 2620)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:3], len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115775, 115775)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = {}\n",
    "nouns_list = []\n",
    "for synset_id in wordnet.all_synsets('n'):\n",
    "    for lemma in synset_id.lemmas():\n",
    "        ltext = lemma.name()\n",
    "        if ltext not in nouns:\n",
    "            nouns_list.append(ltext)\n",
    "        nouns.setdefault(ltext, []).append(synset_id.name())\n",
    "len(nouns), len(nouns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79689"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset2words = {}\n",
    "for synset_id in wordnet.all_synsets('n'):\n",
    "    for lemma in synset_id.lemmas():\n",
    "        synset2words.setdefault(synset_id.name(), []).append(lemma.name())\n",
    "len(synset2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('entity', ['entity.n.01']),\n",
       " ('thing',\n",
       "  ['thing.n.12',\n",
       "   'thing.n.02',\n",
       "   'thing.n.03',\n",
       "   'thing.n.09',\n",
       "   'thing.n.06',\n",
       "   'matter.n.02',\n",
       "   'thing.n.07',\n",
       "   'thing.n.10',\n",
       "   'thing.n.05',\n",
       "   'thing.n.04',\n",
       "   'thing.n.11',\n",
       "   'thing.n.01']),\n",
       " ('anything', ['anything.n.01'])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nouns.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2620, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(data={'word': test})\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['private'] = [1 for x in df_test['word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2620\n",
       "Name: private, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['private'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>private</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ready-made</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rancidness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heterometaboly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type_AB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heartbreaker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  private\n",
       "0      ready-made        1\n",
       "1      rancidness        1\n",
       "2  heterometaboly        1\n",
       "3         type_AB        1\n",
       "4    heartbreaker        1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionarydump = \"D:/dialogue2020/dialogue2020_shared_task_hypernyms/dataset/enwiktionary-latest-pages-articles-multistream.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2doc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114203359it [04:36, 413556.71it/s]\n"
     ]
    }
   ],
   "source": [
    "doc = {}\n",
    "fields = {\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"title\": \"title\",\n",
    "    \"text\": \"text\",\n",
    "    \"redirect title\": \"redirect_title\",\n",
    "}\n",
    "cnt = 0\n",
    "for _, elem in tqdm(ET.iterparse(wiktionarydump, events=(\"end\",))):\n",
    "    prefix, has_namespace, postfix = elem.tag.partition('}')\n",
    "    tag = postfix if postfix else prefix\n",
    "    if tag in fields:\n",
    "        doc[fields[tag]] = elem.text\n",
    "    if tag == \"page\":\n",
    "        elem.clear()\n",
    "        cnt += 1\n",
    "        title2doc[doc[\"title\"]] = doc\n",
    "        doc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest article by lowercased word\n",
    "ltitle2doc = {}\n",
    "for x in title2doc.keys():\n",
    "    if x.lower() in ltitle2doc:\n",
    "        if len(title2doc[x]['text']) > len(ltitle2doc[x.lower()]['text']):\n",
    "            ltitle2doc[x.lower()] = title2doc[x]\n",
    "    else:\n",
    "        ltitle2doc[x.lower()] = title2doc[x]\n",
    "ltitle_list = list(ltitle2doc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest article by lowercased word\n",
    "ltitle2docs = {}\n",
    "for x in title2doc.keys():\n",
    "    ltitle2docs.setdefault(x.lower(), []).append(title2doc[x])\n",
    "ltitle_list = list(ltitle2docs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: wikt_in, dtype: int64)\n",
      "1    1564\n",
      "0    1056\n",
      "Name: wikt_in, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test['wikt_in'] = [1 if x.lower().replace(\"_\", \" \") in ltitle2doc else 0 for x in df_test['word']]\n",
    "print(df_test[df_test['private']==0]['wikt_in'].value_counts())\n",
    "print(df_test[df_test['private']==1]['wikt_in'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ftmodel = fasttext.load_model(\"../../dialogue2020_shared_task_hypernyms/baselines/models/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwords_list = ftmodel.get_words()\n",
    "ftwords = set(ftwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1627124, 2000000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lword2word = {word.lower(): word for word in ftwords_list}\n",
    "len(lword2word), len(ftwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: ft_in, dtype: int64)\n",
      "0    1411\n",
      "1    1209\n",
      "Name: ft_in, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test['ft_in'] = [1 if x.lower().replace(\"_\", \" \") in lword2word else 0 for x in df_test['word']]\n",
    "print(df_test[df_test['private']==0]['ft_in'].value_counts())\n",
    "print(df_test[df_test['private']==1]['ft_in'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 115775/115775 [00:03<00:00, 38332.48it/s]\n"
     ]
    }
   ],
   "source": [
    "nouns_vectors = np.zeros((len(nouns_list), ftmodel.get_dimension()))\n",
    "for i, word in enumerate(tqdm(nouns_list)):\n",
    "    nouns_vectors[i] = ftmodel.get_sentence_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-d4e69c404dca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mltitle_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mltitle_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mftmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mltitle_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mltitle_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mftmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_sentence_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ltitle_vectors = np.zeros((len(ltitle_list), ftmodel.get_dimension()))\n",
    "for i, word in enumerate(tqdm(ltitle_list)):\n",
    "    ltitle_vectors[i] = ftmodel.get_sentence_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similar(vectors, vector, k=1):\n",
    "    res = []\n",
    "    dots = np.dot(vectors, vector)\n",
    "    for i in range(k):\n",
    "        idx = np.argmax(dots)\n",
    "        res.append(idx)\n",
    "        dots[idx] = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "абсорбент абсорбент\n",
      "абсорбент адсорбент\n",
      "абсорбент сорбент\n",
      "абсорбент абсорбирующий\n",
      "абсорбент энтеросорбент\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "lword = public_test[i].lower()\n",
    "idxs = get_top_k_similar(ltitle_vectors, ftmodel.get_sentence_vector(lword), k=5)\n",
    "for idx in idxs:\n",
    "    print(lword, ltitle_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:26<00:00, 86.56it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test['wn_top10'] = [\n",
    "    [nouns_list[x] for x in get_top_k_similar(nouns_vectors, ftmodel.get_sentence_vector(word.lower()), k=10)]\n",
    "    for word in tqdm(df_test['word'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2287, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(data={'word': [x[0].upper() for x in df_test['wn_top10']]})\n",
    "# df_train = pd.DataFrame(data={'word': []})\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:24<00:00, 94.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# skip self\n",
    "df_train['wn_top10'] = [\n",
    "    [nouns_list[x] for x in get_top_k_similar(nouns_vectors, ftmodel.get_sentence_vector(word.lower()), k=11) if nouns_list[x] != word.lower()]\n",
    "    for word in tqdm(df_train['word'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2287/2287 [12:33<00:00,  3.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2287/2287 [12:51<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [\n",
    "    df_test,\n",
    "    df_train\n",
    "]:\n",
    "    df['wikt_top10'] = [\n",
    "        [ltitle_list[x] for x in get_top_k_similar(ltitle_vectors, ftmodel.get_sentence_vector(word.lower()), k=10)]\n",
    "        for word in tqdm(df['word'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_markup(text):\n",
    "    return text.replace(\"[[\", \"\").replace(\"]]\", \"\").replace(\"{{aslinks|\", \"\")\n",
    "\n",
    "def parse_item(text):\n",
    "    items = []\n",
    "    if text.startswith(\"# \") and len(line) > 2:\n",
    "        items.extend([\n",
    "            clean_markup(x).replace(\"?\", \"\").replace(\";\", \"\").replace(\"'\", \"\").strip() \n",
    "            for x in re.split(',|;', text[2:]) if x not in {'-', '?', '—', ''}\n",
    "        ])\n",
    "    return items\n",
    "\n",
    "def parse_translation(trans):\n",
    "    res = {}\n",
    "    for line in trans.split('\\n'):\n",
    "        if line.startswith('|'):\n",
    "            l, r = line.split('=')\n",
    "            res[l[1:]] = r.replace('[[', '').replace(']]', '')\n",
    "    return res\n",
    "\n",
    "def parse_wiktionary(text):\n",
    "    res = {'hypernym': [], 'synonym': [], 'meaning': []}\n",
    "    h1 = \"\"\n",
    "    texts = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if line.startswith(\"= \") and line.endswith(\" =\"):\n",
    "            h1 = line\n",
    "        if h1 == '= {{-ru-}} =':\n",
    "            texts.append(line)\n",
    "    text = \"\\n\".join(texts)\n",
    "    for par in text.split(\"\\n\\n\"):\n",
    "        for h, f in [('==== Гиперонимы ====', 'hypernym'), ('==== Синонимы ====', 'synonym')]:\n",
    "            if h in par:\n",
    "                res[f] = [w for line in par.split(\"\\n\") for w in parse_item(line)]\n",
    "        for h, f in [('==== Значение ====', 'meaning')]:\n",
    "            if h in par:\n",
    "                res[f] = [clean_markup(line[2:]) for line in par.split(\"\\n\") if line.startswith('# ') and len(line) > 2]\n",
    "        if '=== Перевод ===' in par:\n",
    "            res['translation'] = par.replace('=== Перевод ===\\n', '')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:03<00:00, 607.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:02<00:00, 1056.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    df['wikt_hypernyms_text'] = [\n",
    "        parse_wiktionary(ltitle2doc[word.lower()]['text'])['hypernym'] if word.lower() in ltitle2doc else []\n",
    "        for word in tqdm(df['word'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 8741.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 7000.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    df['wikt_top1_hypernyms_text'] = [\n",
    "        parse_wiktionary(ltitle2doc[words[0].lower()]['text'])['hypernym']\n",
    "        for words in tqdm(df['wikt_top10'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2164, 2697)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in df_test['wikt_top1_hypernyms_text']]), sum([len(x) for x in df_train['wikt_top1_hypernyms_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:01<00:00, 1885.78it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 3446.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    res = []\n",
    "    for words in tqdm(df['wikt_top10']):\n",
    "        res_el = []\n",
    "        for doc in ltitle2docs[words[0].lower()]:\n",
    "            res_el.extend(parse_wiktionary(doc['text'])['hypernym'])\n",
    "        res.append(res_el)\n",
    "    df['wikt_top1_hypernyms_text_docs'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2254, 2828)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in df_test['wikt_top1_hypernyms_text_docs']]), sum([len(x) for x in df_train['wikt_top1_hypernyms_text_docs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2287/2287 [00:01<00:00, 1683.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 10260.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 11348.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 14332.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 14791.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 13813.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 12739.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 14425.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 15291.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 13644.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 13812.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 14239.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 15386.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 16052.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 15462.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 15596.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 13254.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 14237.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 15086.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2287/2287 [00:00<00:00, 15598.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    for i in range(10):\n",
    "        df['wn_top%d_hypernyms' % (i + 1)] = [\n",
    "            [hyp for synset_id in nouns[words[i]] for hyp in ruwordnet.get_hypernyms_by_id(synset_id)]\n",
    "            for words in tqdm(df['wn_top10'])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    cnt = 0\n",
    "    lens = []\n",
    "    wikt_top1_hypernyms = []\n",
    "    for word, hypernyms_text in zip(df['word'], df['wikt_top1_hypernyms_text_docs']):\n",
    "        res = []\n",
    "        for hypernym in hypernyms_text:\n",
    "            lhypernym = hypernym.lower().replace('ё', 'е')\n",
    "            if lhypernym in nouns:\n",
    "                res.extend(sorted(nouns[lhypernym]))\n",
    "            else:\n",
    "                parsed = morph.parse(lhypernym)\n",
    "                if 'plur' in parsed[0].tag and parsed[0].normal_form in nouns:\n",
    "                    res.extend(sorted(nouns[parsed[0].normal_form]))\n",
    "                cnt += 1\n",
    "        lens.append(len(res))\n",
    "        wikt_top1_hypernyms.append(res)\n",
    "    df['wikt_top1_hypernyms_docs'] = wikt_top1_hypernyms\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>private</th>\n",
       "      <th>wikt_in</th>\n",
       "      <th>ft_in</th>\n",
       "      <th>wn_top10</th>\n",
       "      <th>wikt_top10</th>\n",
       "      <th>wikt_hypernyms_text</th>\n",
       "      <th>wikt_top1_hypernyms_text</th>\n",
       "      <th>wikt_top1_hypernyms_text_docs</th>\n",
       "      <th>wn_top1_hypernyms</th>\n",
       "      <th>wn_top2_hypernyms</th>\n",
       "      <th>wn_top3_hypernyms</th>\n",
       "      <th>wn_top4_hypernyms</th>\n",
       "      <th>wn_top5_hypernyms</th>\n",
       "      <th>wn_top6_hypernyms</th>\n",
       "      <th>wn_top7_hypernyms</th>\n",
       "      <th>wn_top8_hypernyms</th>\n",
       "      <th>wn_top9_hypernyms</th>\n",
       "      <th>wn_top10_hypernyms</th>\n",
       "      <th>wikt_top1_hypernyms_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>АБДОМИНОПЛАСТИКА</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[ринопластика, лапароскопия, подтяжка, хирурги...</td>\n",
       "      <td>[абдоминопластика, липосакция, блефаропластика...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[144594-N]</td>\n",
       "      <td>[1063-N]</td>\n",
       "      <td>[118172-N, 129710-N, 144594-N]</td>\n",
       "      <td>[1047-N]</td>\n",
       "      <td>[1064-N]</td>\n",
       "      <td>[1047-N]</td>\n",
       "      <td>[108870-N, 110911-N]</td>\n",
       "      <td>[7529-N]</td>\n",
       "      <td>[118698-N, 7529-N]</td>\n",
       "      <td>[5390-N]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>АБСОРБЕНТ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[коагулянт, реагент, растворитель, теплоизолят...</td>\n",
       "      <td>[абсорбент, адсорбент, сорбент, абсорбирующий,...</td>\n",
       "      <td>[сорбент]</td>\n",
       "      <td>[сорбент]</td>\n",
       "      <td>[сорбент]</td>\n",
       "      <td>[113920-N]</td>\n",
       "      <td>[147758-N]</td>\n",
       "      <td>[3675-N]</td>\n",
       "      <td>[2409-N]</td>\n",
       "      <td>[130864-N, 820-N]</td>\n",
       "      <td>[56-N]</td>\n",
       "      <td>[142-N, 146017-N, 8432-N]</td>\n",
       "      <td>[820-N, 106613-N]</td>\n",
       "      <td>[121668-N]</td>\n",
       "      <td>[133238-N, 56-N, 8998-N]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>АВАЛЬ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[акцепт, вексель, аккредитив, тратта, переводн...</td>\n",
       "      <td>[аваль, авалист, авал, акцепт, индоссамент, ве...</td>\n",
       "      <td>[гарантия, поручительство]</td>\n",
       "      <td>[гарантия, поручительство]</td>\n",
       "      <td>[гарантия, поручительство]</td>\n",
       "      <td>[1333-N, 107138-N]</td>\n",
       "      <td>[145804-N, 145983-N, 1792-N, 6424-N]</td>\n",
       "      <td>[1764-N, 457-N, 6424-N]</td>\n",
       "      <td>[1337-N]</td>\n",
       "      <td>[1337-N]</td>\n",
       "      <td>[1278-N, 142951-N]</td>\n",
       "      <td>[121692-N, 566-N, 138373-N]</td>\n",
       "      <td>[116488-N, 1333-N, 151047-N]</td>\n",
       "      <td>[1333-N]</td>\n",
       "      <td>[138167-N]</td>\n",
       "      <td>[147548-N, 150684-N, 9063-N]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>АВТАРКИЯ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[самоизоляция, самодостаточность, суверенизаци...</td>\n",
       "      <td>[автаркия, автаркиям, автаркию, автаркиях, авт...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[113853-N]</td>\n",
       "      <td>[119136-N]</td>\n",
       "      <td>[111489-N]</td>\n",
       "      <td>[113853-N, 1208-N]</td>\n",
       "      <td>[136242-N, 3706-N, 7984-N]</td>\n",
       "      <td>[923-N]</td>\n",
       "      <td>[119563-N, 107258-N]</td>\n",
       "      <td>[125652-N]</td>\n",
       "      <td>[106825-N, 151436-N]</td>\n",
       "      <td>[4895-N, 107374-N, 125550-N]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>АГНОСТИК</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[атеист, материалист, протестант, идеалист, фу...</td>\n",
       "      <td>[агностик, атеист, агностицизм, теист, верующи...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[107524-N]</td>\n",
       "      <td>[122232-N]</td>\n",
       "      <td>[4544-N]</td>\n",
       "      <td>[126711-N, 107524-N, 122232-N]</td>\n",
       "      <td>[127024-N, 4474-N]</td>\n",
       "      <td>[4474-N]</td>\n",
       "      <td>[107524-N]</td>\n",
       "      <td>[4474-N]</td>\n",
       "      <td>[107524-N]</td>\n",
       "      <td>[2149-N]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>АДЖИКА</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[приправа, алыча, кинза, пряная приправа, папр...</td>\n",
       "      <td>[аджика, ткемали, лечо, приправа, чихиртма, ба...</td>\n",
       "      <td>[приправа]</td>\n",
       "      <td>[приправа]</td>\n",
       "      <td>[приправа]</td>\n",
       "      <td>[368-N]</td>\n",
       "      <td>[107956-N, 144253-N, 144322-N, 354-N]</td>\n",
       "      <td>[124049-N, 124081-N, 153883-N]</td>\n",
       "      <td>[107911-N, 370-N, 4681-N]</td>\n",
       "      <td>[107871-N, 348-N, 6878-N]</td>\n",
       "      <td>[350-N, 4789-N, 107911-N, 370-N]</td>\n",
       "      <td>[107641-N, 370-N]</td>\n",
       "      <td>[113199-N]</td>\n",
       "      <td>[107778-N, 106934-N, 106934-N]</td>\n",
       "      <td>[109109-N, 111436-N, 5731-N]</td>\n",
       "      <td>[107911-N]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  private  wikt_in  ft_in  \\\n",
       "0  АБДОМИНОПЛАСТИКА        0        1      1   \n",
       "1         АБСОРБЕНТ        0        1      1   \n",
       "2             АВАЛЬ        0        1      1   \n",
       "3          АВТАРКИЯ        0        1      1   \n",
       "4          АГНОСТИК        0        1      1   \n",
       "5            АДЖИКА        0        1      1   \n",
       "\n",
       "                                            wn_top10  \\\n",
       "0  [ринопластика, лапароскопия, подтяжка, хирурги...   \n",
       "1  [коагулянт, реагент, растворитель, теплоизолят...   \n",
       "2  [акцепт, вексель, аккредитив, тратта, переводн...   \n",
       "3  [самоизоляция, самодостаточность, суверенизаци...   \n",
       "4  [атеист, материалист, протестант, идеалист, фу...   \n",
       "5  [приправа, алыча, кинза, пряная приправа, папр...   \n",
       "\n",
       "                                          wikt_top10  \\\n",
       "0  [абдоминопластика, липосакция, блефаропластика...   \n",
       "1  [абсорбент, адсорбент, сорбент, абсорбирующий,...   \n",
       "2  [аваль, авалист, авал, акцепт, индоссамент, ве...   \n",
       "3  [автаркия, автаркиям, автаркию, автаркиях, авт...   \n",
       "4  [агностик, атеист, агностицизм, теист, верующи...   \n",
       "5  [аджика, ткемали, лечо, приправа, чихиртма, ба...   \n",
       "\n",
       "          wikt_hypernyms_text    wikt_top1_hypernyms_text  \\\n",
       "0                          []                          []   \n",
       "1                   [сорбент]                   [сорбент]   \n",
       "2  [гарантия, поручительство]  [гарантия, поручительство]   \n",
       "3                          []                          []   \n",
       "4                          []                          []   \n",
       "5                  [приправа]                  [приправа]   \n",
       "\n",
       "  wikt_top1_hypernyms_text_docs   wn_top1_hypernyms  \\\n",
       "0                            []          [144594-N]   \n",
       "1                     [сорбент]          [113920-N]   \n",
       "2    [гарантия, поручительство]  [1333-N, 107138-N]   \n",
       "3                            []          [113853-N]   \n",
       "4                            []          [107524-N]   \n",
       "5                    [приправа]             [368-N]   \n",
       "\n",
       "                       wn_top2_hypernyms               wn_top3_hypernyms  \\\n",
       "0                               [1063-N]  [118172-N, 129710-N, 144594-N]   \n",
       "1                             [147758-N]                        [3675-N]   \n",
       "2   [145804-N, 145983-N, 1792-N, 6424-N]         [1764-N, 457-N, 6424-N]   \n",
       "3                             [119136-N]                      [111489-N]   \n",
       "4                             [122232-N]                        [4544-N]   \n",
       "5  [107956-N, 144253-N, 144322-N, 354-N]  [124049-N, 124081-N, 153883-N]   \n",
       "\n",
       "                wn_top4_hypernyms           wn_top5_hypernyms  \\\n",
       "0                        [1047-N]                    [1064-N]   \n",
       "1                        [2409-N]           [130864-N, 820-N]   \n",
       "2                        [1337-N]                    [1337-N]   \n",
       "3              [113853-N, 1208-N]  [136242-N, 3706-N, 7984-N]   \n",
       "4  [126711-N, 107524-N, 122232-N]          [127024-N, 4474-N]   \n",
       "5       [107911-N, 370-N, 4681-N]   [107871-N, 348-N, 6878-N]   \n",
       "\n",
       "                  wn_top6_hypernyms            wn_top7_hypernyms  \\\n",
       "0                          [1047-N]         [108870-N, 110911-N]   \n",
       "1                            [56-N]    [142-N, 146017-N, 8432-N]   \n",
       "2                [1278-N, 142951-N]  [121692-N, 566-N, 138373-N]   \n",
       "3                           [923-N]         [119563-N, 107258-N]   \n",
       "4                          [4474-N]                   [107524-N]   \n",
       "5  [350-N, 4789-N, 107911-N, 370-N]            [107641-N, 370-N]   \n",
       "\n",
       "              wn_top8_hypernyms               wn_top9_hypernyms  \\\n",
       "0                      [7529-N]              [118698-N, 7529-N]   \n",
       "1             [820-N, 106613-N]                      [121668-N]   \n",
       "2  [116488-N, 1333-N, 151047-N]                        [1333-N]   \n",
       "3                    [125652-N]            [106825-N, 151436-N]   \n",
       "4                      [4474-N]                      [107524-N]   \n",
       "5                    [113199-N]  [107778-N, 106934-N, 106934-N]   \n",
       "\n",
       "             wn_top10_hypernyms      wikt_top1_hypernyms_docs  \n",
       "0                      [5390-N]                            []  \n",
       "1      [133238-N, 56-N, 8998-N]                            []  \n",
       "2                    [138167-N]  [147548-N, 150684-N, 9063-N]  \n",
       "3  [4895-N, 107374-N, 125550-N]                            []  \n",
       "4                      [2149-N]                            []  \n",
       "5  [109109-N, 111436-N, 5731-N]                    [107911-N]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>wn_top10</th>\n",
       "      <th>wikt_top10</th>\n",
       "      <th>wikt_hypernyms_text</th>\n",
       "      <th>wikt_top1_hypernyms_text</th>\n",
       "      <th>wikt_top1_hypernyms_text_docs</th>\n",
       "      <th>wn_top1_hypernyms</th>\n",
       "      <th>wn_top2_hypernyms</th>\n",
       "      <th>wn_top3_hypernyms</th>\n",
       "      <th>wn_top4_hypernyms</th>\n",
       "      <th>wn_top5_hypernyms</th>\n",
       "      <th>wn_top6_hypernyms</th>\n",
       "      <th>wn_top7_hypernyms</th>\n",
       "      <th>wn_top8_hypernyms</th>\n",
       "      <th>wn_top9_hypernyms</th>\n",
       "      <th>wn_top10_hypernyms</th>\n",
       "      <th>wikt_top1_hypernyms_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>РИНОПЛАСТИКА</td>\n",
       "      <td>[хирургия, подтяжка, пластическая хирургия, пл...</td>\n",
       "      <td>[ринопластика, отопластика, блефаропластика, а...</td>\n",
       "      <td>[пластика]</td>\n",
       "      <td>[пластика]</td>\n",
       "      <td>[пластика]</td>\n",
       "      <td>[1047-N]</td>\n",
       "      <td>[118172-N, 129710-N, 144594-N]</td>\n",
       "      <td>[1064-N]</td>\n",
       "      <td>[5390-N]</td>\n",
       "      <td>[118698-N, 7529-N]</td>\n",
       "      <td>[113873-N, 125596-N]</td>\n",
       "      <td>[7529-N]</td>\n",
       "      <td>[1063-N]</td>\n",
       "      <td>[123844-N, 5390-N]</td>\n",
       "      <td>[1063-N]</td>\n",
       "      <td>[113873-N, 118951-N]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>КОАГУЛЯНТ</td>\n",
       "      <td>[реагент, коллоид, растворитель, раствор, конц...</td>\n",
       "      <td>[коагулянт, флокулянт, коагулятор, минерализат...</td>\n",
       "      <td>[химикат, лекарство, медикамент]</td>\n",
       "      <td>[химикат, лекарство, медикамент]</td>\n",
       "      <td>[химикат, лекарство, медикамент]</td>\n",
       "      <td>[147758-N]</td>\n",
       "      <td>[115242-N]</td>\n",
       "      <td>[3675-N]</td>\n",
       "      <td>[61-N, 109704-N, 3675-N, 111752-N]</td>\n",
       "      <td>[106507-N, 820-N, 154715-N, 3393-N]</td>\n",
       "      <td>[820-N]</td>\n",
       "      <td>[109704-N, 3675-N]</td>\n",
       "      <td>[131273-N, 135863-N, 461-N]</td>\n",
       "      <td>[109873-N, 2445-N, 56-N]</td>\n",
       "      <td>[56-N]</td>\n",
       "      <td>[6232-N, 1067-N, 1067-N]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>АКЦЕПТ</td>\n",
       "      <td>[акцепт оферты, акцепт плательщика, акцепт без...</td>\n",
       "      <td>[акцепт, акцептант, акцептовать, индоссамент, ...</td>\n",
       "      <td>[принятие, подтверждение, подпись]</td>\n",
       "      <td>[принятие, подтверждение, подпись]</td>\n",
       "      <td>[принятие, подтверждение, подпись]</td>\n",
       "      <td>[107138-N]</td>\n",
       "      <td>[1333-N]</td>\n",
       "      <td>[107138-N]</td>\n",
       "      <td>[1764-N, 457-N, 6424-N]</td>\n",
       "      <td>[145804-N, 145983-N, 1792-N, 6424-N]</td>\n",
       "      <td>[118698-N]</td>\n",
       "      <td>[1337-N]</td>\n",
       "      <td>[150944-N, 1554-N]</td>\n",
       "      <td>[112280-N]</td>\n",
       "      <td>[124851-N, 130027-N, 1333-N]</td>\n",
       "      <td>[124852-N, 134035-N, 146793-N, 106870-N, 13746...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>САМОИЗОЛЯЦИЯ</td>\n",
       "      <td>[маргинализация, изоляция, милитаризация, конф...</td>\n",
       "      <td>[самоизоляция, самоизоляцию, самоизоляции, мар...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[139989-N]</td>\n",
       "      <td>[2409-N, 107417-N, 106595-N, 107325-N]</td>\n",
       "      <td>[1527-N, 833-N]</td>\n",
       "      <td>[107213-N]</td>\n",
       "      <td>[118839-N]</td>\n",
       "      <td>[113853-N, 1208-N]</td>\n",
       "      <td>[137916-N, 143193-N]</td>\n",
       "      <td>[106633-N]</td>\n",
       "      <td>[111489-N]</td>\n",
       "      <td>[151843-N]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>АТЕИСТ</td>\n",
       "      <td>[сектант, христианин, безбожник, протестант, к...</td>\n",
       "      <td>[атеист, теист, верующий, сектант, христианин,...</td>\n",
       "      <td>[человек]</td>\n",
       "      <td>[человек]</td>\n",
       "      <td>[человек]</td>\n",
       "      <td>[4474-N]</td>\n",
       "      <td>[4474-N]</td>\n",
       "      <td>[107524-N]</td>\n",
       "      <td>[4544-N]</td>\n",
       "      <td>[4544-N]</td>\n",
       "      <td>[122232-N]</td>\n",
       "      <td>[107524-N]</td>\n",
       "      <td>[107524-N]</td>\n",
       "      <td>[4544-N]</td>\n",
       "      <td>[4474-N]</td>\n",
       "      <td>[2149-N]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ПРИПРАВА</td>\n",
       "      <td>[пряная приправа, специя, пряность, паприка, к...</td>\n",
       "      <td>[приправа, специя, пряность, аджика, специи, п...</td>\n",
       "      <td>[пищевая добавка, добавление, добавка]</td>\n",
       "      <td>[пищевая добавка, добавление, добавка]</td>\n",
       "      <td>[пищевая добавка, добавление, добавка]</td>\n",
       "      <td>[107911-N, 370-N, 4681-N]</td>\n",
       "      <td>[107911-N, 370-N, 4681-N, 368-N]</td>\n",
       "      <td>[107911-N, 370-N, 4681-N, 119371-N, 119844-N]</td>\n",
       "      <td>[107871-N, 348-N, 6878-N]</td>\n",
       "      <td>[124049-N, 124081-N, 153883-N]</td>\n",
       "      <td>[107778-N, 106934-N, 106934-N]</td>\n",
       "      <td>[350-N, 4789-N, 107911-N, 370-N]</td>\n",
       "      <td>[106509-N]</td>\n",
       "      <td>[107911-N]</td>\n",
       "      <td>[107911-N]</td>\n",
       "      <td>[111983-N, 118759-N, 130864-N, 138267-N, 13086...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word                                           wn_top10  \\\n",
       "0  РИНОПЛАСТИКА  [хирургия, подтяжка, пластическая хирургия, пл...   \n",
       "1     КОАГУЛЯНТ  [реагент, коллоид, растворитель, раствор, конц...   \n",
       "2        АКЦЕПТ  [акцепт оферты, акцепт плательщика, акцепт без...   \n",
       "3  САМОИЗОЛЯЦИЯ  [маргинализация, изоляция, милитаризация, конф...   \n",
       "4        АТЕИСТ  [сектант, христианин, безбожник, протестант, к...   \n",
       "5      ПРИПРАВА  [пряная приправа, специя, пряность, паприка, к...   \n",
       "\n",
       "                                          wikt_top10  \\\n",
       "0  [ринопластика, отопластика, блефаропластика, а...   \n",
       "1  [коагулянт, флокулянт, коагулятор, минерализат...   \n",
       "2  [акцепт, акцептант, акцептовать, индоссамент, ...   \n",
       "3  [самоизоляция, самоизоляцию, самоизоляции, мар...   \n",
       "4  [атеист, теист, верующий, сектант, христианин,...   \n",
       "5  [приправа, специя, пряность, аджика, специи, п...   \n",
       "\n",
       "                      wikt_hypernyms_text  \\\n",
       "0                              [пластика]   \n",
       "1        [химикат, лекарство, медикамент]   \n",
       "2      [принятие, подтверждение, подпись]   \n",
       "3                                      []   \n",
       "4                               [человек]   \n",
       "5  [пищевая добавка, добавление, добавка]   \n",
       "\n",
       "                 wikt_top1_hypernyms_text  \\\n",
       "0                              [пластика]   \n",
       "1        [химикат, лекарство, медикамент]   \n",
       "2      [принятие, подтверждение, подпись]   \n",
       "3                                      []   \n",
       "4                               [человек]   \n",
       "5  [пищевая добавка, добавление, добавка]   \n",
       "\n",
       "            wikt_top1_hypernyms_text_docs          wn_top1_hypernyms  \\\n",
       "0                              [пластика]                   [1047-N]   \n",
       "1        [химикат, лекарство, медикамент]                 [147758-N]   \n",
       "2      [принятие, подтверждение, подпись]                 [107138-N]   \n",
       "3                                      []                 [139989-N]   \n",
       "4                               [человек]                   [4474-N]   \n",
       "5  [пищевая добавка, добавление, добавка]  [107911-N, 370-N, 4681-N]   \n",
       "\n",
       "                        wn_top2_hypernyms  \\\n",
       "0          [118172-N, 129710-N, 144594-N]   \n",
       "1                              [115242-N]   \n",
       "2                                [1333-N]   \n",
       "3  [2409-N, 107417-N, 106595-N, 107325-N]   \n",
       "4                                [4474-N]   \n",
       "5        [107911-N, 370-N, 4681-N, 368-N]   \n",
       "\n",
       "                               wn_top3_hypernyms  \\\n",
       "0                                       [1064-N]   \n",
       "1                                       [3675-N]   \n",
       "2                                     [107138-N]   \n",
       "3                                [1527-N, 833-N]   \n",
       "4                                     [107524-N]   \n",
       "5  [107911-N, 370-N, 4681-N, 119371-N, 119844-N]   \n",
       "\n",
       "                    wn_top4_hypernyms                     wn_top5_hypernyms  \\\n",
       "0                            [5390-N]                    [118698-N, 7529-N]   \n",
       "1  [61-N, 109704-N, 3675-N, 111752-N]   [106507-N, 820-N, 154715-N, 3393-N]   \n",
       "2             [1764-N, 457-N, 6424-N]  [145804-N, 145983-N, 1792-N, 6424-N]   \n",
       "3                          [107213-N]                            [118839-N]   \n",
       "4                            [4544-N]                              [4544-N]   \n",
       "5           [107871-N, 348-N, 6878-N]        [124049-N, 124081-N, 153883-N]   \n",
       "\n",
       "                wn_top6_hypernyms                 wn_top7_hypernyms  \\\n",
       "0            [113873-N, 125596-N]                          [7529-N]   \n",
       "1                         [820-N]                [109704-N, 3675-N]   \n",
       "2                      [118698-N]                          [1337-N]   \n",
       "3              [113853-N, 1208-N]              [137916-N, 143193-N]   \n",
       "4                      [122232-N]                        [107524-N]   \n",
       "5  [107778-N, 106934-N, 106934-N]  [350-N, 4789-N, 107911-N, 370-N]   \n",
       "\n",
       "             wn_top8_hypernyms         wn_top9_hypernyms  \\\n",
       "0                     [1063-N]        [123844-N, 5390-N]   \n",
       "1  [131273-N, 135863-N, 461-N]  [109873-N, 2445-N, 56-N]   \n",
       "2           [150944-N, 1554-N]                [112280-N]   \n",
       "3                   [106633-N]                [111489-N]   \n",
       "4                   [107524-N]                  [4544-N]   \n",
       "5                   [106509-N]                [107911-N]   \n",
       "\n",
       "             wn_top10_hypernyms  \\\n",
       "0                      [1063-N]   \n",
       "1                        [56-N]   \n",
       "2  [124851-N, 130027-N, 1333-N]   \n",
       "3                    [151843-N]   \n",
       "4                      [4474-N]   \n",
       "5                    [107911-N]   \n",
       "\n",
       "                            wikt_top1_hypernyms_docs  \n",
       "0                               [113873-N, 118951-N]  \n",
       "1                           [6232-N, 1067-N, 1067-N]  \n",
       "2  [124852-N, 134035-N, 146793-N, 106870-N, 13746...  \n",
       "3                                                 []  \n",
       "4                                           [2149-N]  \n",
       "5  [111983-N, 118759-N, 130864-N, 138267-N, 13086...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def save_to_file(words_with_hypernyms, output_path, ruwordnet):\n",
    "    with codecs.open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for word, hypernyms in words_with_hypernyms.items():\n",
    "            for hypernym in hypernyms:\n",
    "                f.write(f\"{word}\\t{hypernym}\\t{ruwordnet.get_name_by_id(hypernym)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_hypernyms(l, sz=10):\n",
    "    res_set = set()\n",
    "    res = []\n",
    "    for el in sorted(l):\n",
    "        if el[1] not in res_set:\n",
    "            res.append(el[1])\n",
    "        res_set.add(el[1])\n",
    "    return res[:sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_train some items added to hypernyms multiple times\n",
    "features = {word: {} for word in chain(df_test['word'], df_train['word'])}\n",
    "hypernyms = {word: [] for word in chain(df_test['word'], df_train['word'])}\n",
    "\n",
    "syn_priority_l1 = 4.\n",
    "syn_priority_l2 = 2.\n",
    "syn_priority_l3 = 3.\n",
    "syntail_priority_l1 = 7.\n",
    "syntail_priority_l2 = 5.\n",
    "syntail_priority_l3 = 6.\n",
    "wikhyp_priority2_l1 = 0.\n",
    "wikhyp_priority2_l2 = 1.\n",
    "wikhyp_priority3_l1 = 5.\n",
    "wikhyp_priority3_l2 = 6.\n",
    "\n",
    "for df in [df_test, df_train]:\n",
    "    for word, hs in zip(df['word'], df['wikt_top1_hypernyms_docs']):\n",
    "        for j, hypernym in enumerate(hs):\n",
    "            features[word].setdefault(hypernym, {})['wikhyp_priority_l1'] = 1\n",
    "            features[word].setdefault(hypernym, {})['wikhyp_priority_l1_pos'] = j\n",
    "            for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                features[word].setdefault(hyphyp, {})['wikhyp_priority_l2'] = 1\n",
    "                features[word].setdefault(hyphyp, {})['wikhyp_priority_l2_pos'] = j\n",
    "        for j, hypernym in enumerate(hs[:2]):\n",
    "            hypernyms[word].append((wikhyp_priority2_l1 + j*1e-3, hypernym))\n",
    "            for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                hypernyms[word].append((wikhyp_priority2_l2 + j*1e-3, hyphyp))\n",
    "        for j, hypernym in enumerate(hs[2:]):\n",
    "            hypernyms[word].append((wikhyp_priority3_l1 + j*1e-3, hypernym))\n",
    "            for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                hypernyms[word].append((wikhyp_priority3_l2 + j*1e-3, hyphyp))\n",
    "\n",
    "    for i in range(2, 11):\n",
    "        for word, hs in zip(df['word'], df['wn_top%d_hypernyms' % i]):\n",
    "            for j, hypernym in enumerate(hs):\n",
    "                features[word].setdefault(hypernym, {})['syn%d_priority_l2'%i] = 1\n",
    "                features[word].setdefault(hypernym, {})['syn%d_priority_l2_pos'%i] = j\n",
    "                hypernyms[word].append((syntail_priority_l2 + (i-2)*1e-3, hypernym))\n",
    "                for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                    features[word].setdefault(hyphyp, {})['syn%d_priority_l3'%i] = 1\n",
    "                    features[word].setdefault(hyphyp, {})['syn%d_priority_l3_pos'%i] = j\n",
    "                    hypernyms[word].append((syntail_priority_l3 + (i-2)*1e-3, hyphyp))\n",
    "\n",
    "    for word, hs in zip(df['word'], df['wn_top1_hypernyms']):\n",
    "        for j, hypernym in enumerate(hs):\n",
    "            hypernyms[word].append((syn_priority_l2, hypernym))\n",
    "            features[word].setdefault(hypernym, {})['syn1_priority_l2'] = 1\n",
    "            features[word].setdefault(hypernym, {})['syn1_priority_l2_pos'] = j\n",
    "            for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                features[word].setdefault(hyphyp, {})['syn1_priority_l3'] = 1\n",
    "                features[word].setdefault(hyphyp, {})['syn1_priority_l3_pos'] = j\n",
    "                hypernyms[word].append((syn_priority_l3, hyphyp))\n",
    "\n",
    "    for word, words in zip(df['word'], df['wn_top10']):\n",
    "        for synset_id in nouns[words[0]]:\n",
    "            hypernyms[word].append((syn_priority_l1, synset_id))\n",
    "            features[word].setdefault(synset_id, {})['syn1_priority_l1'] = 1\n",
    "            features[word].setdefault(synset_id, {})['syn1_priority_l1_pos'] = 0\n",
    "        for i, word2 in enumerate(words[1:]):\n",
    "            for j, synset_id in enumerate(nouns[word2]):\n",
    "                hypernyms[word].append((syntail_priority_l1 + i*1e-3, synset_id))\n",
    "                features[word].setdefault(synset_id, {})['syn%d_priority_l1'%(i+2)] = 1\n",
    "                features[word].setdefault(synset_id, {})['syn%d_priority_l1_pos'%(i+2)] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "try:\n",
    "    wn.all_synsets\n",
    "except LookupError as e:\n",
    "    import nltk\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_trailing_dot(s):\n",
    "    if s.endswith('.'):\n",
    "        return s[:-1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4697"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru2en = {}\n",
    "with open('data/ru.txt', 'r', encoding='utf-8') as f_ru, open('data/en_ya.txt', 'r', encoding='utf-8') as f_en_y:\n",
    "    for i, r, ey in zip(range(100500), f_ru, f_en_y):\n",
    "        r = drop_trailing_dot(r.strip())\n",
    "        ey = drop_trailing_dot(ey.strip())\n",
    "        ru2en[r] = ey\n",
    "len(ru2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8481"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ru = {}\n",
    "with open('data/hyp_en.txt', 'r', encoding=\"utf-8\") as f_en, open('data/hyp_ru_ya.txt', 'r', encoding=\"utf-8\") as f_ru_y:\n",
    "    for i, e, ruy, in zip(range(100500), f_en, f_ru_y):\n",
    "        e = drop_trailing_dot(e.strip())\n",
    "        ruy = drop_trailing_dot(ruy.strip())\n",
    "        en2ru[e] = ruy\n",
    "len(en2ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5736773062642426\n",
      "0.6974202008319107\n"
     ]
    }
   ],
   "source": [
    "missing = set()\n",
    "\n",
    "hypernyms_en = {}\n",
    "hypernyms_en_txt = {}\n",
    "for df in [df_test, df_train]:\n",
    "    cnt = 0\n",
    "    for word in df[\"word\"]:\n",
    "        hypernyms_en[word] = set()\n",
    "        hypernyms_en_txt[word] = set()\n",
    "        lword = word.lower()\n",
    "        if lword in ru2en:\n",
    "            synsets = wn.synsets(ru2en[lword])\n",
    "            if synsets:\n",
    "                flag = False\n",
    "                for sense in synsets:\n",
    "                    for hyp in sense.hypernyms():\n",
    "                        for name in hyp.lemma_names():\n",
    "                            name = name.replace('_', ' ')\n",
    "                            if name in en2ru:\n",
    "                                if en2ru[name].lower() in nouns:\n",
    "                                    flag = True\n",
    "                                    hypernyms_en_txt[word].add(en2ru[name].lower())\n",
    "                                    for id_ in nouns[en2ru[name].lower()]:\n",
    "                                        hypernyms[word].append((0.0, id_))\n",
    "                                        hypernyms_en[word].add((0.0, id_))\n",
    "                            else:\n",
    "                                missing.add(name)\n",
    "            if hypernyms_en[word]:\n",
    "                cnt += 1\n",
    "    print(cnt / (len(df[\"word\"])+1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "АБДОМИНОПЛАСТИКА {'пластическая хирургия'} {(0.0, '10123-N')}\n",
      "АБСОРБЕНТ set() set()\n",
      "АБСЕНТЕИЗМ {'отсутствие'} {(0.0, '114377-N')}\n",
      "АБСОЛЮТИЗАЦИЯ set() set()\n"
     ]
    }
   ],
   "source": [
    "for word in public_test[:2] + private_test[:2]:\n",
    "    print(word, hypernyms_en_txt[word], hypernyms_en[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in hypernyms_en:\n",
    "    for score, hypernym in hypernyms_en[word]:\n",
    "        features[word].setdefault(hypernym, {})['wordnet_en_l1'] = 1\n",
    "        for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "            features[word].setdefault(hyphyp, {})['wordnet_en_l2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_words = {}\n",
    "def normalize(s):\n",
    "    res = []\n",
    "    for word in word_tokenize(s.lower()):\n",
    "        if word in norm_words:\n",
    "            res.append(norm_words[word])\n",
    "        else:\n",
    "            mp = morph.parse(word)\n",
    "            if mp:\n",
    "                norm_words[word] = mp[0].normal_form\n",
    "                res.append(norm_words[word])\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innertext(tag):\n",
    "    return (tag.text or '') + ''.join(innertext(e) for e in tag) + (tag.tail or '')\n",
    "\n",
    "def get_serp_texts(xml_path, k=5):\n",
    "    root = ET.parse(xml_path).getroot()\n",
    "    res = []\n",
    "    for e in root.find('response').find('results').find('grouping').findall('group')[:k]:\n",
    "        res.append(innertext(e.find('doc').find('title')))\n",
    "        if e.find('doc').find('passages'):\n",
    "            for passage in e.find('doc').find('passages'):\n",
    "                res.append(innertext(passage))\n",
    "        if e.find('doc').find('headline'):\n",
    "            res.append(innertext(e.find('doc').find('headline')))\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2287/2287 [02:54<00:00, 12.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2287/2287 [03:32<00:00, 12.03it/s]\n"
     ]
    }
   ],
   "source": [
    "synset_norm_serp_ya_cnt = Counter()\n",
    "synset_norm_serp_g_cnt = Counter()\n",
    "hypernyms_wserp = {}\n",
    "serp_priority = -4.\n",
    "serp_hyp_priority = -4.\n",
    "meaning_priority = -1.\n",
    "\n",
    "for df in [\n",
    "    df_test,\n",
    "    df_train\n",
    "]:\n",
    "    for word in tqdm(df[\"word\"]):\n",
    "        word_file_path = 'data/google_it_all/' + word.lower() + '.tsv'\n",
    "        total_g_serp = \"\"\n",
    "        if os.path.exists(word_file_path):\n",
    "            with open(word_file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    text = line.split('\\t')[1]\n",
    "                    total_g_serp += text + \" \"\n",
    "        norm_total_g_serp = normalize(total_g_serp)\n",
    "        word_file_path = 'data/yandex_it_all/' + word.upper() + '.xml'\n",
    "        total_ya_serp = get_serp_texts(word_file_path, k=10)\n",
    "        norm_total_ya_serp = normalize(total_ya_serp)\n",
    "\n",
    "        total_meaning = \"\"\n",
    "        if word.lower() in ltitle2doc:\n",
    "            for meaning in parse_wiktionary(ltitle2doc[word.lower()]['text'])['meaning']:\n",
    "                total_meaning += meaning + \" \"\n",
    "        norm_total_meaning = normalize(total_meaning)\n",
    "\n",
    "        res = []\n",
    "        for score, hypernym in hypernyms[word]:\n",
    "            hypernym_texts = synset2words[hypernym] + [ruwordnet.get_name_by_id(hypernym)]\n",
    "            for hypernym_text in hypernym_texts:\n",
    "                norm_hypernym_text = normalize(hypernym_text)\n",
    "                if norm_hypernym_text in norm_total_g_serp:\n",
    "                    score += serp_priority\n",
    "                    features[word][hypernym]['serp_g_norm'] = 1\n",
    "                    for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                        if hyphyp in features[word]:\n",
    "                            features[word][hyphyp]['serp_g_norm_l2'] = 1\n",
    "                    synset_norm_serp_g_cnt[hypernym] += 1\n",
    "                if hypernym_text in total_g_serp:\n",
    "                    features[word][hypernym]['serp_g'] = 1\n",
    "\n",
    "                if norm_hypernym_text in norm_total_ya_serp:\n",
    "                    features[word][hypernym]['serp_ya_norm'] = 1\n",
    "                    synset_norm_serp_ya_cnt[hypernym] += 1\n",
    "                    for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                        if hyphyp in features[word]:\n",
    "                            features[word][hyphyp]['serp_ya_norm_l2'] = 1\n",
    "                if hypernym_text in total_ya_serp:\n",
    "                    features[word][hypernym]['serp_ya'] = 1\n",
    "\n",
    "                if norm_hypernym_text in norm_total_meaning:\n",
    "                    score += meaning_priority\n",
    "                    features[word][hypernym]['meaning_norm'] = 1\n",
    "                if hypernym_text in total_meaning:\n",
    "                    features[word][hypernym]['meaning'] = 1\n",
    "            res.append((score, hypernym))\n",
    "        hypernyms_wserp[word] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = set()\n",
    "# for word in features:\n",
    "#     for synset_id in features[word]:\n",
    "#         for key in features[word][synset_id]:\n",
    "#             feature_names.add(key)\n",
    "# feature_names = sorted(feature_names)\n",
    "# len(feature_names)\n",
    "feature_names = [\n",
    "    \"meaning\", \n",
    "    \"meaning_norm\", \n",
    "    #\"serp_g\", \n",
    "    #\"serp_g_norm\", \n",
    "    #\"serp_g_norm_l2\", \n",
    "    #\"serp_ya\", \n",
    "    #\"serp_ya_norm\", \n",
    "    #\"serp_ya_norm_l2\", \n",
    "    \"syn1_priority_l1\", \n",
    "    \"syn1_priority_l2\", \n",
    "    \"syn1_priority_l3\", \n",
    "    \"syn2_priority_l1\", \n",
    "    \"syn2_priority_l2\", \n",
    "    \"syn2_priority_l3\", \n",
    "    \"syn3_priority_l1\", \n",
    "    \"syn3_priority_l2\", \n",
    "    \"syn3_priority_l3\", \n",
    "    \"syn4_priority_l1\", \n",
    "    \"syn4_priority_l2\", \n",
    "    \"syn4_priority_l3\", \n",
    "    \"syn5_priority_l1\", \n",
    "    \"syn5_priority_l2\", \n",
    "    \"syn5_priority_l3\", \n",
    "    \"syn6_priority_l1\", \n",
    "    \"syn6_priority_l2\", \n",
    "    \"syn6_priority_l3\", \n",
    "    \"syn7_priority_l1\", \n",
    "    \"syn7_priority_l2\", \n",
    "    \"syn7_priority_l3\", \n",
    "    \"wikhyp_priority_l1\", \n",
    "    \"wikhyp_priority_l2\", \n",
    "    \"wordnet_en_l1\", \n",
    "    \"wordnet_en_l2\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96760, 27), (96760,))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = sum([len(features[x]) for x in df_train[\"word\"]])\n",
    "X = np.zeros( (total, len(feature_names)) )\n",
    "y = np.zeros( total )\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96760"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = 0\n",
    "for word in df_train[\"word\"]:\n",
    "    lword = word.lower()\n",
    "    true_hypernyms = set()\n",
    "    for synset_id in nouns[lword]:\n",
    "        true_hypernyms.update(ruwordnet.get_hypernyms_by_id(synset_id))\n",
    "    for synset_id in features[word]:\n",
    "        y[pos] = 1 if synset_id in true_hypernyms else 0\n",
    "        X[pos] = [features[word][synset_id].get(fn, 0) for fn in feature_names]\n",
    "        pos += 1\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((77408,), (19352,))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.9029802250177584\n",
      "0.003 0.9077366085408979\n",
      "0.01 0.911201851469948\n",
      "0.03 0.912018361937933\n",
      "0.1 0.91180640529166\n",
      "0.3 0.9111811522803481\n",
      "1.0 0.9111699243066536\n",
      "3 0.911072844343622\n",
      "10 0.9110729971051688\n",
      "30 0.911077809093895\n",
      "100 0.9110788020439494\n",
      "300 0.9110792603285901\n",
      "1000 0.9110792603285901\n"
     ]
    }
   ],
   "source": [
    "for C in [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3, 10, 30, 100, 300, 1000]:\n",
    "    model = LogisticRegression(C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(C, roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.03, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.03)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {k:v for k,v in zip(feature_names, model.coef_[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = {}\n",
    "for i in range(1, 11):\n",
    "    for j in range(1,4):\n",
    "        pos_weights['syn%d_priority_l%d_pos' % (i, j)] = -0.0001\n",
    "\n",
    "def calc_score(d):\n",
    "    score = 0.\n",
    "    for feature, weight in weights.items():\n",
    "        score += weight * d.get(feature, 0)\n",
    "    for feature, weight in pos_weights.items():\n",
    "        score += weight * d.get(feature, 0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'subm/subm106_no_gya'\n",
    "save_to_file({\n",
    "    k: get_top_hypernyms( [(-calc_score(features[k][x]), x) for x in features[k]] )\n",
    "    for k in public_test\n",
    "}, prefix + '_public.tsv', ruwordnet)\n",
    "save_to_file({\n",
    "    k: get_top_hypernyms( [(-calc_score(features[k][x]), x) for x in features[k]] )\n",
    "    for k in private_test\n",
    "}, prefix + '_private.tsv', ruwordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"meaning\": 0.15,\n",
    "    \"meaning_norm\": 0.15,\n",
    "    \"serp_g\": 0.20,\n",
    "    \"serp_g_norm\": 0.50,\n",
    "    \"serp_g_norm_l2\": 0.20,\n",
    "    \"serp_ya\": 0.15,\n",
    "    \"serp_ya_norm\": 0.20,\n",
    "    \"serp_ya_norm_l2\": 0.20,\n",
    "    \n",
    "    \"syn1_priority_l1\": 0.05,\n",
    "    \"syn1_priority_l2\": 0.50,\n",
    "    \"syn1_priority_l3\": 0.30,\n",
    "    \n",
    "    \"syn2_priority_l1\": 0.05,\n",
    "    \"syn2_priority_l2\": 0.25,\n",
    "    \"syn2_priority_l3\": 0.15,\n",
    "    \n",
    "    \"syn3_priority_l1\": 0.05,\n",
    "    \"syn3_priority_l2\": 0.25,\n",
    "    \"syn3_priority_l3\": 0.15,\n",
    "    \n",
    "    \"syn4_priority_l1\": 0.00,\n",
    "    \"syn4_priority_l2\": 0.10,\n",
    "    \"syn4_priority_l3\": 0.15,\n",
    "    \n",
    "    \"syn5_priority_l1\": 0.00,\n",
    "    \"syn5_priority_l2\": 0.10,\n",
    "    \"syn5_priority_l3\": 0.00,\n",
    "    \n",
    "    \"syn6_priority_l1\": 0.00,\n",
    "    \"syn6_priority_l2\": 0.10,\n",
    "    \"syn6_priority_l3\": 0.00,\n",
    "    \n",
    "    \"syn7_priority_l1\": 0.00,\n",
    "    \"syn7_priority_l2\": 0.10,\n",
    "    \"syn7_priority_l3\": 0.00,\n",
    "    \n",
    "    \"wikhyp_priority_l1\": 0.20,\n",
    "    \"wikhyp_priority_l2\": 0.50,\n",
    "    \n",
    "    \"wordnet_en_l1\": 0.05,\n",
    "    \"wordnet_en_l2\": 0.20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'subm/subm107_no_gya'\n",
    "save_to_file({\n",
    "    k: get_top_hypernyms( [(-calc_score(features[k][x]), x) for x in features[k]] )\n",
    "    for k in public_test\n",
    "}, prefix + '_public.tsv', ruwordnet)\n",
    "save_to_file({\n",
    "    k: get_top_hypernyms( [(-calc_score(features[k][x]), x) for x in features[k]] )\n",
    "    for k in private_test\n",
    "}, prefix + '_private.tsv', ruwordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
