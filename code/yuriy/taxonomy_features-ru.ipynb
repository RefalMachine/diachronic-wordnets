{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../taxonomy-enrichment/baselines/ruwordnet')\n",
    "sys.path.append('../../taxonomy-enrichment/baselines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "from ruwordnet_reader import RuWordnet\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruwordnet = RuWordnet(db_path=\"../../dialogue2020_shared_task_hypernyms/dataset/ruwordnet.db\", ruwordnet_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test = []\n",
    "with open('../../dialogue2020_shared_task_hypernyms/dataset/public/verbs_public_no_labels.tsv', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        public_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test = []\n",
    "with open('../../dialogue2020_shared_task_hypernyms/dataset/private/verbs_private_no_labels.tsv', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        private_test.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['АБСОЛЮТИЗИРОВАТЬ', 'АКТИВИРОВАТЬ', 'АМЕРИКАНИЗИРОВАТЬ'],\n",
       " ['АДСОРБИРОВАТЬ', 'АКАТЬ', 'АКТИРОВАТЬ'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_test[:3], private_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26538, 26538)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = {}\n",
    "nouns_list = []\n",
    "for sense_id, synset_id, text in ruwordnet.get_all_senses():\n",
    "    if synset_id.endswith(\"V\"):\n",
    "        ltext = text.lower()\n",
    "        if ltext not in nouns:\n",
    "            nouns_list.append(ltext)\n",
    "        nouns.setdefault(ltext, []).append(synset_id)\n",
    "len(nouns), len(nouns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7521"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset2words = {}\n",
    "for sense_id, synset_id, text in ruwordnet.get_all_senses():\n",
    "    if synset_id.endswith(\"V\"):\n",
    "        synset2words.setdefault(synset_id, []).append(text.lower())\n",
    "len(synset2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('льет дождь', ['4223-V']),\n",
       " ('дождь льет', ['4223-V']),\n",
       " ('лить как из ведра', ['4223-V'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nouns.items())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(data={'word': public_test + private_test})\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['private'] = [1 if x in private_test else 0 for x in df_test['word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    350\n",
       "0    175\n",
       "Name: private, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['private'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>private</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>АБСОЛЮТИЗИРОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>АКТИВИРОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>АМЕРИКАНИЗИРОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>АНОДИРОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>БИНТОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  private\n",
       "0   АБСОЛЮТИЗИРОВАТЬ        0\n",
       "1       АКТИВИРОВАТЬ        0\n",
       "2  АМЕРИКАНИЗИРОВАТЬ        0\n",
       "3        АНОДИРОВАТЬ        0\n",
       "4          БИНТОВАТЬ        0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionarydump = \"ruwiktionary-20200120-pages-articles-multistream.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2doc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35866269it [02:25, 246403.81it/s]\n"
     ]
    }
   ],
   "source": [
    "doc = {}\n",
    "fields = {\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"title\": \"title\",\n",
    "    \"text\": \"text\",\n",
    "    \"redirect title\": \"redirect_title\",\n",
    "}\n",
    "cnt = 0\n",
    "for _, elem in tqdm(ET.iterparse(wiktionarydump, events=(\"end\",))):\n",
    "    prefix, has_namespace, postfix = elem.tag.partition('}')\n",
    "    tag = postfix if postfix else prefix\n",
    "    if tag in fields:\n",
    "        doc[fields[tag]] = elem.text\n",
    "    if tag == \"page\":\n",
    "        elem.clear()\n",
    "        cnt += 1\n",
    "        title2doc[doc[\"title\"]] = doc\n",
    "        doc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest article by lowercased word\n",
    "ltitle2doc = {}\n",
    "for x in title2doc.keys():\n",
    "    if x.lower() in ltitle2doc:\n",
    "        if len(title2doc[x]['text']) > len(ltitle2doc[x.lower()]['text']):\n",
    "            ltitle2doc[x.lower()] = title2doc[x]\n",
    "    else:\n",
    "        ltitle2doc[x.lower()] = title2doc[x]\n",
    "ltitle_list = list(ltitle2doc.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest article by lowercased word\n",
    "ltitle2docs = {}\n",
    "for x in title2doc.keys():\n",
    "    ltitle2docs.setdefault(x.lower(), []).append(title2doc[x])\n",
    "ltitle_list = list(ltitle2docs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    173\n",
      "0      2\n",
      "Name: wikt_in, dtype: int64\n",
      "1    350\n",
      "Name: wikt_in, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test['wikt_in'] = [1 if x.lower() in ltitle2doc else 0 for x in df_test['word']]\n",
    "print(df_test[df_test['private']==0]['wikt_in'].value_counts())\n",
    "print(df_test[df_test['private']==1]['wikt_in'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ftmodel = fasttext.load_model(\"../../dialogue2020_shared_task_hypernyms/baselines/models/cc.ru.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwords_list = ftmodel.get_words()\n",
    "ftwords = set(ftwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1674899, 2000000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lword2word = {word.lower(): word for word in ftwords_list}\n",
    "len(lword2word), len(ftwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    140\n",
      "0     35\n",
      "Name: ft_in, dtype: int64\n",
      "1    279\n",
      "0     71\n",
      "Name: ft_in, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test['ft_in'] = [1 if x.lower() in lword2word else 0 for x in df_test['word']]\n",
    "print(df_test[df_test['private']==0]['ft_in'].value_counts())\n",
    "print(df_test[df_test['private']==1]['ft_in'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 26538/26538 [00:01<00:00, 21649.73it/s]\n"
     ]
    }
   ],
   "source": [
    "nouns_vectors = np.zeros((len(nouns_list), ftmodel.get_dimension()))\n",
    "for i, word in enumerate(tqdm(nouns_list)):\n",
    "    nouns_vectors[i] = ftmodel.get_sentence_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 2177428/2177428 [00:53<00:00, 41035.03it/s]\n"
     ]
    }
   ],
   "source": [
    "ltitle_vectors = np.zeros((len(ltitle_list), ftmodel.get_dimension()))\n",
    "for i, word in enumerate(tqdm(ltitle_list)):\n",
    "    ltitle_vectors[i] = ftmodel.get_sentence_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similar(vectors, vector, k=1):\n",
    "    res = []\n",
    "    dots = np.dot(vectors, vector)\n",
    "    for i in range(k):\n",
    "        idx = np.argmax(dots)\n",
    "        res.append(idx)\n",
    "        dots[idx] = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "активировать активировать\n",
      "активировать деактивировать\n",
      "активировать активироваться\n",
      "активировать реактивировать\n",
      "активировать разблокировать\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "lword = public_test[i].lower()\n",
    "idxs = get_top_k_similar(ltitle_vectors, ftmodel.get_sentence_vector(lword), k=5)\n",
    "for idx in idxs:\n",
    "    print(lword, ltitle_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 525/525 [00:02<00:00, 231.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test['wn_top10'] = [\n",
    "    [nouns_list[x] for x in get_top_k_similar(nouns_vectors, ftmodel.get_sentence_vector(word.lower()), k=10)]\n",
    "    for word in tqdm(df_test['word'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(data={'word': [x[0].upper() for x in df_test['wn_top10']]})\n",
    "# df_train = pd.DataFrame(data={'word': []})\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 525/525 [00:02<00:00, 247.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# skip self\n",
    "df_train['wn_top10'] = [\n",
    "    [nouns_list[x] for x in get_top_k_similar(nouns_vectors, ftmodel.get_sentence_vector(word.lower()), k=11) if nouns_list[x] != word.lower()]\n",
    "    for word in tqdm(df_train['word'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 525/525 [02:51<00:00,  3.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 525/525 [02:51<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [\n",
    "    df_test,\n",
    "    df_train\n",
    "]:\n",
    "    df['wikt_top10'] = [\n",
    "        [ltitle_list[x] for x in get_top_k_similar(ltitle_vectors, ftmodel.get_sentence_vector(word.lower()), k=10)]\n",
    "        for word in tqdm(df['word'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_markup(text):\n",
    "    return text.replace(\"[[\", \"\").replace(\"]]\", \"\").replace(\"{{aslinks|\", \"\")\n",
    "\n",
    "def parse_item(text):\n",
    "    items = []\n",
    "    if text.startswith(\"# \") and len(line) > 2:\n",
    "        items.extend([\n",
    "            clean_markup(x).replace(\"?\", \"\").replace(\";\", \"\").replace(\"'\", \"\").strip() \n",
    "            for x in re.split(',|;', text[2:]) if x not in {'-', '?', '—', ''}\n",
    "        ])\n",
    "    return items\n",
    "\n",
    "def parse_translation(trans):\n",
    "    res = {}\n",
    "    for line in trans.split('\\n'):\n",
    "        if line.startswith('|'):\n",
    "            l, r = line.split('=')\n",
    "            res[l[1:]] = r.replace('[[', '').replace(']]', '')\n",
    "    return res\n",
    "\n",
    "def parse_wiktionary(text):\n",
    "    res = {'hypernym': [], 'synonym': [], 'meaning': []}\n",
    "    h1 = \"\"\n",
    "    texts = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if line.startswith(\"= \") and line.endswith(\" =\"):\n",
    "            h1 = line\n",
    "        if h1 == '= {{-ru-}} =':\n",
    "            texts.append(line)\n",
    "    text = \"\\n\".join(texts)\n",
    "    for par in text.split(\"\\n\\n\"):\n",
    "        for h, f in [('==== Гиперонимы ====', 'hypernym'), ('==== Синонимы ====', 'synonym')]:\n",
    "            if h in par:\n",
    "                res[f] = [w for line in par.split(\"\\n\") for w in parse_item(line)]\n",
    "        for h, f in [('==== Значение ====', 'meaning')]:\n",
    "            if h in par:\n",
    "                res[f] = [clean_markup(line[2:]) for line in par.split(\"\\n\") if line.startswith('# ') and len(line) > 2]\n",
    "        if '=== Перевод ===' in par:\n",
    "            res['translation'] = par.replace('=== Перевод ===\\n', '')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 638.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 917.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    df['wikt_hypernyms_text'] = [\n",
    "        parse_wiktionary(ltitle2doc[word.lower()]['text'])['hypernym'] if word.lower() in ltitle2doc else []\n",
    "        for word in tqdm(df['word'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 9571.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 8808.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    df['wikt_top1_hypernyms_text'] = [\n",
    "        parse_wiktionary(ltitle2doc[words[0].lower()]['text'])['hypernym']\n",
    "        for words in tqdm(df['wikt_top10'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 211)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in df_test['wikt_top1_hypernyms_text']]), sum([len(x) for x in df_train['wikt_top1_hypernyms_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 1789.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 1636.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    res = []\n",
    "    for words in tqdm(df['wikt_top10']):\n",
    "        res_el = []\n",
    "        for doc in ltitle2docs[words[0].lower()]:\n",
    "            res_el.extend(parse_wiktionary(doc['text'])['hypernym'])\n",
    "        res.append(res_el)\n",
    "    df['wikt_top1_hypernyms_text_docs'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 211)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in df_test['wikt_top1_hypernyms_text_docs']]), sum([len(x) for x in df_train['wikt_top1_hypernyms_text_docs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 903.38it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 6430.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 6011.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 8231.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 10154.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 11475.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 11697.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 8128.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 9748.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 10967.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 11698.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 12241.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 11443.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 10647.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 11956.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 7050.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 8663.16it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 11932.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 11832.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 525/525 [00:00<00:00, 12239.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    for i in range(10):\n",
    "        df['wn_top%d_hypernyms' % (i + 1)] = [\n",
    "            [hyp for synset_id in nouns[words[i]] for hyp in ruwordnet.get_hypernyms_by_id(synset_id)]\n",
    "            for words in tqdm(df['wn_top10'])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "for df in [df_test, df_train]:\n",
    "    cnt = 0\n",
    "    lens = []\n",
    "    wikt_top1_hypernyms = []\n",
    "    for word, hypernyms_text in zip(df['word'], df['wikt_top1_hypernyms_text_docs']):\n",
    "        res = []\n",
    "        for hypernym in hypernyms_text:\n",
    "            lhypernym = hypernym.lower().replace('ё', 'е')\n",
    "            if lhypernym in nouns:\n",
    "                res.extend(sorted(nouns[lhypernym]))\n",
    "            else:\n",
    "                parsed = morph.parse(lhypernym)\n",
    "                if 'plur' in parsed[0].tag and parsed[0].normal_form in nouns:\n",
    "                    res.extend(sorted(nouns[parsed[0].normal_form]))\n",
    "                cnt += 1\n",
    "        lens.append(len(res))\n",
    "        wikt_top1_hypernyms.append(res)\n",
    "    df['wikt_top1_hypernyms_docs'] = wikt_top1_hypernyms\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>private</th>\n",
       "      <th>wikt_in</th>\n",
       "      <th>ft_in</th>\n",
       "      <th>wn_top10</th>\n",
       "      <th>wikt_top10</th>\n",
       "      <th>wikt_hypernyms_text</th>\n",
       "      <th>wikt_top1_hypernyms_text</th>\n",
       "      <th>wikt_top1_hypernyms_text_docs</th>\n",
       "      <th>wn_top1_hypernyms</th>\n",
       "      <th>wn_top2_hypernyms</th>\n",
       "      <th>wn_top3_hypernyms</th>\n",
       "      <th>wn_top4_hypernyms</th>\n",
       "      <th>wn_top5_hypernyms</th>\n",
       "      <th>wn_top6_hypernyms</th>\n",
       "      <th>wn_top7_hypernyms</th>\n",
       "      <th>wn_top8_hypernyms</th>\n",
       "      <th>wn_top9_hypernyms</th>\n",
       "      <th>wn_top10_hypernyms</th>\n",
       "      <th>wikt_top1_hypernyms_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>АБСОЛЮТИЗИРОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[преувеличивать, переоценивать, идеализировать...</td>\n",
       "      <td>[абсолютизировать, абсолютизируя, абсолютизиро...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[116115-V, 139096-V, 107602-V, 115033-V, 11503...</td>\n",
       "      <td>[116751-V]</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[106882-V, 106517-V]</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[115032-V, 131407-V, 132922-V, 116112-V, 13909...</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[117017-V, 107137-V, 115247-V]</td>\n",
       "      <td>[117017-V, 106875-V, 118713-V]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>АКТИВИРОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[разблокировать, дезактивировать, отключить, з...</td>\n",
       "      <td>[активировать, деактивировать, активироваться,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[153231-V, 121460-V]</td>\n",
       "      <td>[144157-V, 4661-V, 6444-V]</td>\n",
       "      <td>[107417-V, 111842-V]</td>\n",
       "      <td>[111022-V, 120174-V, 106473-V, 107441-V, 10744...</td>\n",
       "      <td>[111668-V, 110751-V, 111668-V]</td>\n",
       "      <td>[106631-V, 106638-V, 106531-V, 107417-V]</td>\n",
       "      <td>[149898-V, 106490-V, 106490-V, 106493-V, 13284...</td>\n",
       "      <td>[106585-V, 106698-V, 106709-V, 118698-V, 10649...</td>\n",
       "      <td>[106533-V, 106704-V, 106882-V, 116223-V, 11956...</td>\n",
       "      <td>[110474-V]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>АМЕРИКАНИЗИРОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[урбанизировать, гофрировать, индустриализиров...</td>\n",
       "      <td>[американизировать, американизироваться, амери...</td>\n",
       "      <td>[изменять]</td>\n",
       "      <td>[изменять]</td>\n",
       "      <td>[изменять]</td>\n",
       "      <td>[112075-V, 116640-V]</td>\n",
       "      <td>[124516-V]</td>\n",
       "      <td>[923-V]</td>\n",
       "      <td>[135851-V]</td>\n",
       "      <td>[106501-V, 7237-V]</td>\n",
       "      <td>[106501-V, 7237-V]</td>\n",
       "      <td>[106494-V, 145128-V]</td>\n",
       "      <td>[121336-V, 107410-V]</td>\n",
       "      <td>[116636-V]</td>\n",
       "      <td>[107417-V]</td>\n",
       "      <td>[106631-V, 111281-V, 117315-V]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>АНОДИРОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[газировать, гофрировать, оцинковать, наполиро...</td>\n",
       "      <td>[анодировать, анодироваться, анодировав, аноди...</td>\n",
       "      <td>[обрабатывать]</td>\n",
       "      <td>[обрабатывать]</td>\n",
       "      <td>[обрабатывать]</td>\n",
       "      <td>[111435-V]</td>\n",
       "      <td>[124516-V]</td>\n",
       "      <td>[107325-V]</td>\n",
       "      <td>[111769-V, 111785-V, 146751-V]</td>\n",
       "      <td>[107325-V]</td>\n",
       "      <td>[106494-V, 145128-V]</td>\n",
       "      <td>[106950-V]</td>\n",
       "      <td>[107325-V]</td>\n",
       "      <td>[111109-V]</td>\n",
       "      <td>[107325-V]</td>\n",
       "      <td>[106534-V, 106535-V]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>БИНТОВАТЬ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[перевязывать, заматывать, обматывать, растира...</td>\n",
       "      <td>[бинтовать, перебинтовать, забинтовать, бинтов...</td>\n",
       "      <td>[перевязывать, обматывать, обёртывать]</td>\n",
       "      <td>[перевязывать, обматывать, обёртывать]</td>\n",
       "      <td>[перевязывать, обматывать, обёртывать]</td>\n",
       "      <td>[111427-V, 111467-V, 120969-V, 116334-V]</td>\n",
       "      <td>[114599-V, 118663-V, 112000-V]</td>\n",
       "      <td>[107325-V, 112000-V]</td>\n",
       "      <td>[111781-V, 115942-V, 118570-V]</td>\n",
       "      <td>[107325-V, 110813-V, 115187-V, 106888-V]</td>\n",
       "      <td>[107011-V, 114441-V]</td>\n",
       "      <td>[106472-V, 106529-V]</td>\n",
       "      <td>[111427-V, 111467-V, 120969-V, 116334-V]</td>\n",
       "      <td>[108856-V]</td>\n",
       "      <td>[111427-V]</td>\n",
       "      <td>[111428-V, 117117-V, 127479-V, 115187-V, 11623...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>БОДРИТЬСЯ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[бодрить, ободриться, приободриться, храбритьс...</td>\n",
       "      <td>[бодриться, бодрить, ободриться, приободриться...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[118574-V]</td>\n",
       "      <td>[107253-V, 125736-V]</td>\n",
       "      <td>[107253-V, 125736-V]</td>\n",
       "      <td>[115900-V]</td>\n",
       "      <td>[118574-V]</td>\n",
       "      <td>[106714-V, 119928-V, 106484-V, 106531-V, 14940...</td>\n",
       "      <td>[106565-V, 123840-V, 148270-V]</td>\n",
       "      <td>[128286-V, 116244-V, 124452-V]</td>\n",
       "      <td>[120955-V]</td>\n",
       "      <td>[107253-V, 125736-V]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  private  wikt_in  ft_in  \\\n",
       "0   АБСОЛЮТИЗИРОВАТЬ        0        1      1   \n",
       "1       АКТИВИРОВАТЬ        0        1      1   \n",
       "2  АМЕРИКАНИЗИРОВАТЬ        0        1      0   \n",
       "3        АНОДИРОВАТЬ        0        1      0   \n",
       "4          БИНТОВАТЬ        0        1      1   \n",
       "5          БОДРИТЬСЯ        0        1      1   \n",
       "\n",
       "                                            wn_top10  \\\n",
       "0  [преувеличивать, переоценивать, идеализировать...   \n",
       "1  [разблокировать, дезактивировать, отключить, з...   \n",
       "2  [урбанизировать, гофрировать, индустриализиров...   \n",
       "3  [газировать, гофрировать, оцинковать, наполиро...   \n",
       "4  [перевязывать, заматывать, обматывать, растира...   \n",
       "5  [бодрить, ободриться, приободриться, храбритьс...   \n",
       "\n",
       "                                          wikt_top10  \\\n",
       "0  [абсолютизировать, абсолютизируя, абсолютизиро...   \n",
       "1  [активировать, деактивировать, активироваться,...   \n",
       "2  [американизировать, американизироваться, амери...   \n",
       "3  [анодировать, анодироваться, анодировав, аноди...   \n",
       "4  [бинтовать, перебинтовать, забинтовать, бинтов...   \n",
       "5  [бодриться, бодрить, ободриться, приободриться...   \n",
       "\n",
       "                      wikt_hypernyms_text  \\\n",
       "0                                      []   \n",
       "1                                      []   \n",
       "2                              [изменять]   \n",
       "3                          [обрабатывать]   \n",
       "4  [перевязывать, обматывать, обёртывать]   \n",
       "5                                      []   \n",
       "\n",
       "                 wikt_top1_hypernyms_text  \\\n",
       "0                                      []   \n",
       "1                                      []   \n",
       "2                              [изменять]   \n",
       "3                          [обрабатывать]   \n",
       "4  [перевязывать, обматывать, обёртывать]   \n",
       "5                                      []   \n",
       "\n",
       "            wikt_top1_hypernyms_text_docs  \\\n",
       "0                                      []   \n",
       "1                                      []   \n",
       "2                              [изменять]   \n",
       "3                          [обрабатывать]   \n",
       "4  [перевязывать, обматывать, обёртывать]   \n",
       "5                                      []   \n",
       "\n",
       "                          wn_top1_hypernyms  \\\n",
       "0                                [116390-V]   \n",
       "1                      [153231-V, 121460-V]   \n",
       "2                      [112075-V, 116640-V]   \n",
       "3                                [111435-V]   \n",
       "4  [111427-V, 111467-V, 120969-V, 116334-V]   \n",
       "5                                [118574-V]   \n",
       "\n",
       "                                   wn_top2_hypernyms     wn_top3_hypernyms  \\\n",
       "0  [116115-V, 139096-V, 107602-V, 115033-V, 11503...            [116751-V]   \n",
       "1                         [144157-V, 4661-V, 6444-V]  [107417-V, 111842-V]   \n",
       "2                                         [124516-V]               [923-V]   \n",
       "3                                         [124516-V]            [107325-V]   \n",
       "4                     [114599-V, 118663-V, 112000-V]  [107325-V, 112000-V]   \n",
       "5                               [107253-V, 125736-V]  [107253-V, 125736-V]   \n",
       "\n",
       "                                   wn_top4_hypernyms  \\\n",
       "0                                         [116390-V]   \n",
       "1  [111022-V, 120174-V, 106473-V, 107441-V, 10744...   \n",
       "2                                         [135851-V]   \n",
       "3                     [111769-V, 111785-V, 146751-V]   \n",
       "4                     [111781-V, 115942-V, 118570-V]   \n",
       "5                                         [115900-V]   \n",
       "\n",
       "                          wn_top5_hypernyms  \\\n",
       "0                      [106882-V, 106517-V]   \n",
       "1            [111668-V, 110751-V, 111668-V]   \n",
       "2                        [106501-V, 7237-V]   \n",
       "3                                [107325-V]   \n",
       "4  [107325-V, 110813-V, 115187-V, 106888-V]   \n",
       "5                                [118574-V]   \n",
       "\n",
       "                                   wn_top6_hypernyms  \\\n",
       "0                                         [116390-V]   \n",
       "1           [106631-V, 106638-V, 106531-V, 107417-V]   \n",
       "2                                 [106501-V, 7237-V]   \n",
       "3                               [106494-V, 145128-V]   \n",
       "4                               [107011-V, 114441-V]   \n",
       "5  [106714-V, 119928-V, 106484-V, 106531-V, 14940...   \n",
       "\n",
       "                                   wn_top7_hypernyms  \\\n",
       "0  [115032-V, 131407-V, 132922-V, 116112-V, 13909...   \n",
       "1  [149898-V, 106490-V, 106490-V, 106493-V, 13284...   \n",
       "2                               [106494-V, 145128-V]   \n",
       "3                                         [106950-V]   \n",
       "4                               [106472-V, 106529-V]   \n",
       "5                     [106565-V, 123840-V, 148270-V]   \n",
       "\n",
       "                                   wn_top8_hypernyms  \\\n",
       "0                                         [116390-V]   \n",
       "1  [106585-V, 106698-V, 106709-V, 118698-V, 10649...   \n",
       "2                               [121336-V, 107410-V]   \n",
       "3                                         [107325-V]   \n",
       "4           [111427-V, 111467-V, 120969-V, 116334-V]   \n",
       "5                     [128286-V, 116244-V, 124452-V]   \n",
       "\n",
       "                                   wn_top9_hypernyms  \\\n",
       "0                     [117017-V, 107137-V, 115247-V]   \n",
       "1  [106533-V, 106704-V, 106882-V, 116223-V, 11956...   \n",
       "2                                         [116636-V]   \n",
       "3                                         [111109-V]   \n",
       "4                                         [108856-V]   \n",
       "5                                         [120955-V]   \n",
       "\n",
       "               wn_top10_hypernyms  \\\n",
       "0  [117017-V, 106875-V, 118713-V]   \n",
       "1                      [110474-V]   \n",
       "2                      [107417-V]   \n",
       "3                      [107325-V]   \n",
       "4                      [111427-V]   \n",
       "5            [107253-V, 125736-V]   \n",
       "\n",
       "                            wikt_top1_hypernyms_docs  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                     [106631-V, 111281-V, 117315-V]  \n",
       "3                               [106534-V, 106535-V]  \n",
       "4  [111428-V, 117117-V, 127479-V, 115187-V, 11623...  \n",
       "5                                                 []  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>wn_top10</th>\n",
       "      <th>wikt_top10</th>\n",
       "      <th>wikt_hypernyms_text</th>\n",
       "      <th>wikt_top1_hypernyms_text</th>\n",
       "      <th>wikt_top1_hypernyms_text_docs</th>\n",
       "      <th>wn_top1_hypernyms</th>\n",
       "      <th>wn_top2_hypernyms</th>\n",
       "      <th>wn_top3_hypernyms</th>\n",
       "      <th>wn_top4_hypernyms</th>\n",
       "      <th>wn_top5_hypernyms</th>\n",
       "      <th>wn_top6_hypernyms</th>\n",
       "      <th>wn_top7_hypernyms</th>\n",
       "      <th>wn_top8_hypernyms</th>\n",
       "      <th>wn_top9_hypernyms</th>\n",
       "      <th>wn_top10_hypernyms</th>\n",
       "      <th>wikt_top1_hypernyms_docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ПРЕУВЕЛИЧИВАТЬ</td>\n",
       "      <td>[преуменьшать, приуменьшать, недооценивать, пе...</td>\n",
       "      <td>[преувеличивать, преуменьшать, приуменьшать, н...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[115032-V, 131407-V, 132922-V, 116112-V, 13909...</td>\n",
       "      <td>[116115-V, 139096-V, 107602-V, 115033-V, 11503...</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[116115-V, 141697-V, 106632-V, 111769-V]</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[116751-V]</td>\n",
       "      <td>[116390-V]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>РАЗБЛОКИРОВАТЬ</td>\n",
       "      <td>[заблокировать, блокировать, разблокировать ус...</td>\n",
       "      <td>[разблокировать, заблокировать, разблокировать...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[106533-V, 106704-V, 106882-V, 116223-V, 11956...</td>\n",
       "      <td>[106533-V, 106704-V, 106882-V, 116223-V, 11956...</td>\n",
       "      <td>[121460-V]</td>\n",
       "      <td>[121458-V, 115580-V]</td>\n",
       "      <td>[107417-V, 111842-V]</td>\n",
       "      <td>[106710-V, 117028-V, 117371-V, 106938-V, 10647...</td>\n",
       "      <td>[106631-V, 129710-V, 110447-V, 117535-V]</td>\n",
       "      <td>[144157-V, 4661-V, 6444-V]</td>\n",
       "      <td>[111022-V, 120174-V, 106473-V, 107441-V, 10744...</td>\n",
       "      <td>[107409-V, 107410-V, 108870-V]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>УРБАНИЗИРОВАТЬ</td>\n",
       "      <td>[гофрировать, милитаризировать, индустриализир...</td>\n",
       "      <td>[урбанизировать, урбанизироваться, урбанизиров...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[124516-V]</td>\n",
       "      <td>[106501-V, 7237-V]</td>\n",
       "      <td>[923-V]</td>\n",
       "      <td>[106501-V, 7237-V]</td>\n",
       "      <td>[3513-V]</td>\n",
       "      <td>[106501-V, 7237-V]</td>\n",
       "      <td>[923-V]</td>\n",
       "      <td>[121336-V, 5944-V]</td>\n",
       "      <td>[106501-V]</td>\n",
       "      <td>[106501-V, 147138-V]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ГАЗИРОВАТЬ</td>\n",
       "      <td>[гофрировать, минерализовать, дегазировать, ка...</td>\n",
       "      <td>[газировать, газироваться, разгазировать, гази...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[124516-V]</td>\n",
       "      <td>[111435-V]</td>\n",
       "      <td>[110908-V, 128431-V]</td>\n",
       "      <td>[106494-V, 145128-V]</td>\n",
       "      <td>[107325-V]</td>\n",
       "      <td>[107325-V]</td>\n",
       "      <td>[114041-V, 115570-V]</td>\n",
       "      <td>[112144-V, 924-V]</td>\n",
       "      <td>[111682-V, 112820-V]</td>\n",
       "      <td>[106674-V, 115814-V]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ПЕРЕВЯЗЫВАТЬ</td>\n",
       "      <td>[перевязать, зашивать, обвязывать, перевязыват...</td>\n",
       "      <td>[перевязывать, перевязать, зашивать, обвязыват...</td>\n",
       "      <td>[лечить, связывать]</td>\n",
       "      <td>[лечить, связывать]</td>\n",
       "      <td>[лечить, связывать]</td>\n",
       "      <td>[111427-V, 111467-V, 120969-V, 116334-V]</td>\n",
       "      <td>[106557-V, 113291-V, 111568-V]</td>\n",
       "      <td>[115187-V]</td>\n",
       "      <td>[111427-V]</td>\n",
       "      <td>[107325-V, 112000-V]</td>\n",
       "      <td>[114599-V, 118663-V, 112000-V]</td>\n",
       "      <td>[116817-V, 127237-V]</td>\n",
       "      <td>[107412-V, 107416-V, 107410-V, 107417-V]</td>\n",
       "      <td>[106557-V]</td>\n",
       "      <td>[110790-V, 113291-V, 106483-V, 106548-V]</td>\n",
       "      <td>[1061-V, 118177-V, 111465-V, 125093-V, 137319-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>БОДРИТЬ</td>\n",
       "      <td>[взбадривать, взбодрить, тонизировать, отрезвл...</td>\n",
       "      <td>[бодрить, взбадривать, бодриться, взбодрить, т...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[118574-V]</td>\n",
       "      <td>[118574-V]</td>\n",
       "      <td>[118574-V, 106632-V]</td>\n",
       "      <td>[128286-V, 116244-V, 124452-V]</td>\n",
       "      <td>[115104-V]</td>\n",
       "      <td>[118574-V]</td>\n",
       "      <td>[115104-V]</td>\n",
       "      <td>[106535-V, 120557-V, 120649-V]</td>\n",
       "      <td>[106817-V, 129710-V, 111493-V, 111942-V]</td>\n",
       "      <td>[114652-V]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word                                           wn_top10  \\\n",
       "0  ПРЕУВЕЛИЧИВАТЬ  [преуменьшать, приуменьшать, недооценивать, пе...   \n",
       "1  РАЗБЛОКИРОВАТЬ  [заблокировать, блокировать, разблокировать ус...   \n",
       "2  УРБАНИЗИРОВАТЬ  [гофрировать, милитаризировать, индустриализир...   \n",
       "3      ГАЗИРОВАТЬ  [гофрировать, минерализовать, дегазировать, ка...   \n",
       "4    ПЕРЕВЯЗЫВАТЬ  [перевязать, зашивать, обвязывать, перевязыват...   \n",
       "5         БОДРИТЬ  [взбадривать, взбодрить, тонизировать, отрезвл...   \n",
       "\n",
       "                                          wikt_top10  wikt_hypernyms_text  \\\n",
       "0  [преувеличивать, преуменьшать, приуменьшать, н...                   []   \n",
       "1  [разблокировать, заблокировать, разблокировать...                   []   \n",
       "2  [урбанизировать, урбанизироваться, урбанизиров...                   []   \n",
       "3  [газировать, газироваться, разгазировать, гази...                   []   \n",
       "4  [перевязывать, перевязать, зашивать, обвязыват...  [лечить, связывать]   \n",
       "5  [бодрить, взбадривать, бодриться, взбодрить, т...                   []   \n",
       "\n",
       "  wikt_top1_hypernyms_text wikt_top1_hypernyms_text_docs  \\\n",
       "0                       []                            []   \n",
       "1                       []                            []   \n",
       "2                       []                            []   \n",
       "3                       []                            []   \n",
       "4      [лечить, связывать]           [лечить, связывать]   \n",
       "5                       []                            []   \n",
       "\n",
       "                                   wn_top1_hypernyms  \\\n",
       "0                                         [116390-V]   \n",
       "1  [106533-V, 106704-V, 106882-V, 116223-V, 11956...   \n",
       "2                                         [124516-V]   \n",
       "3                                         [124516-V]   \n",
       "4           [111427-V, 111467-V, 120969-V, 116334-V]   \n",
       "5                                         [118574-V]   \n",
       "\n",
       "                                   wn_top2_hypernyms  \\\n",
       "0                                         [116390-V]   \n",
       "1  [106533-V, 106704-V, 106882-V, 116223-V, 11956...   \n",
       "2                                 [106501-V, 7237-V]   \n",
       "3                                         [111435-V]   \n",
       "4                     [106557-V, 113291-V, 111568-V]   \n",
       "5                                         [118574-V]   \n",
       "\n",
       "                                   wn_top3_hypernyms  \\\n",
       "0  [115032-V, 131407-V, 132922-V, 116112-V, 13909...   \n",
       "1                                         [121460-V]   \n",
       "2                                            [923-V]   \n",
       "3                               [110908-V, 128431-V]   \n",
       "4                                         [115187-V]   \n",
       "5                               [118574-V, 106632-V]   \n",
       "\n",
       "                                   wn_top4_hypernyms     wn_top5_hypernyms  \\\n",
       "0  [116115-V, 139096-V, 107602-V, 115033-V, 11503...            [116390-V]   \n",
       "1                               [121458-V, 115580-V]  [107417-V, 111842-V]   \n",
       "2                                 [106501-V, 7237-V]              [3513-V]   \n",
       "3                               [106494-V, 145128-V]            [107325-V]   \n",
       "4                                         [111427-V]  [107325-V, 112000-V]   \n",
       "5                     [128286-V, 116244-V, 124452-V]            [115104-V]   \n",
       "\n",
       "                                   wn_top6_hypernyms  \\\n",
       "0                                         [116390-V]   \n",
       "1  [106710-V, 117028-V, 117371-V, 106938-V, 10647...   \n",
       "2                                 [106501-V, 7237-V]   \n",
       "3                                         [107325-V]   \n",
       "4                     [114599-V, 118663-V, 112000-V]   \n",
       "5                                         [118574-V]   \n",
       "\n",
       "                          wn_top7_hypernyms  \\\n",
       "0  [116115-V, 141697-V, 106632-V, 111769-V]   \n",
       "1  [106631-V, 129710-V, 110447-V, 117535-V]   \n",
       "2                                   [923-V]   \n",
       "3                      [114041-V, 115570-V]   \n",
       "4                      [116817-V, 127237-V]   \n",
       "5                                [115104-V]   \n",
       "\n",
       "                          wn_top8_hypernyms  \\\n",
       "0                                [116390-V]   \n",
       "1                [144157-V, 4661-V, 6444-V]   \n",
       "2                        [121336-V, 5944-V]   \n",
       "3                         [112144-V, 924-V]   \n",
       "4  [107412-V, 107416-V, 107410-V, 107417-V]   \n",
       "5            [106535-V, 120557-V, 120649-V]   \n",
       "\n",
       "                                   wn_top9_hypernyms  \\\n",
       "0                                         [116751-V]   \n",
       "1  [111022-V, 120174-V, 106473-V, 107441-V, 10744...   \n",
       "2                                         [106501-V]   \n",
       "3                               [111682-V, 112820-V]   \n",
       "4                                         [106557-V]   \n",
       "5           [106817-V, 129710-V, 111493-V, 111942-V]   \n",
       "\n",
       "                         wn_top10_hypernyms  \\\n",
       "0                                [116390-V]   \n",
       "1            [107409-V, 107410-V, 108870-V]   \n",
       "2                      [106501-V, 147138-V]   \n",
       "3                      [106674-V, 115814-V]   \n",
       "4  [110790-V, 113291-V, 106483-V, 106548-V]   \n",
       "5                                [114652-V]   \n",
       "\n",
       "                            wikt_top1_hypernyms_docs  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4  [1061-V, 118177-V, 111465-V, 125093-V, 137319-...  \n",
       "5                                                 []  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def save_to_file(words_with_hypernyms, output_path, ruwordnet):\n",
    "    with codecs.open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for word, hypernyms in words_with_hypernyms.items():\n",
    "            for hypernym in hypernyms:\n",
    "                f.write(f\"{word}\\t{hypernym}\\t{ruwordnet.get_name_by_id(hypernym)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_hypernyms(l, sz=10):\n",
    "    res_set = set()\n",
    "    res = []\n",
    "    for el in sorted(l):\n",
    "        if el[1] not in res_set:\n",
    "            res.append(el[1])\n",
    "        res_set.add(el[1])\n",
    "    return res[:sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_train some items added to hypernyms multiple times\n",
    "features = {word: {} for word in chain(df_test['word'], df_train['word'])}\n",
    "hypernyms = {word: [] for word in chain(df_test['word'], df_train['word'])}\n",
    "\n",
    "syn_priority_l1 = 4.\n",
    "syn_priority_l2 = 2.\n",
    "syn_priority_l3 = 3.\n",
    "syntail_priority_l1 = 7.\n",
    "syntail_priority_l2 = 5.\n",
    "syntail_priority_l3 = 6.\n",
    "wikhyp_priority2_l1 = 0.\n",
    "wikhyp_priority2_l2 = 1.\n",
    "wikhyp_priority3_l1 = 5.\n",
    "wikhyp_priority3_l2 = 6.\n",
    "\n",
    "for df in [df_test, df_train]:\n",
    "    for word, hs in zip(df['word'], df['wikt_top1_hypernyms_docs']):\n",
    "        for j, hypernym in enumerate(hs):\n",
    "            features[word].setdefault(hypernym, {})['wikhyp_priority_l1'] = 1\n",
    "            features[word].setdefault(hypernym, {})['wikhyp_priority_l1_pos'] = j\n",
    "            for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                features[word].setdefault(hyphyp, {})['wikhyp_priority_l2'] = 1\n",
    "                features[word].setdefault(hyphyp, {})['wikhyp_priority_l2_pos'] = j\n",
    "        for j, hypernym in enumerate(hs[:2]):\n",
    "            hypernyms[word].append((wikhyp_priority2_l1 + j*1e-3, hypernym))\n",
    "            for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                hypernyms[word].append((wikhyp_priority2_l2 + j*1e-3, hyphyp))\n",
    "        for j, hypernym in enumerate(hs[2:]):\n",
    "            hypernyms[word].append((wikhyp_priority3_l1 + j*1e-3, hypernym))\n",
    "            for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                hypernyms[word].append((wikhyp_priority3_l2 + j*1e-3, hyphyp))\n",
    "\n",
    "    for i in range(2, 11):\n",
    "        for word, hs in zip(df['word'], df['wn_top%d_hypernyms' % i]):\n",
    "            for j, hypernym in enumerate(hs):\n",
    "                features[word].setdefault(hypernym, {})['syn%d_priority_l2'%i] = 1\n",
    "                features[word].setdefault(hypernym, {})['syn%d_priority_l2_pos'%i] = j\n",
    "                hypernyms[word].append((syntail_priority_l2 + (i-2)*1e-3, hypernym))\n",
    "                for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                    features[word].setdefault(hyphyp, {})['syn%d_priority_l3'%i] = 1\n",
    "                    features[word].setdefault(hyphyp, {})['syn%d_priority_l3_pos'%i] = j\n",
    "                    hypernyms[word].append((syntail_priority_l3 + (i-2)*1e-3, hyphyp))\n",
    "\n",
    "    for word, hs in zip(df['word'], df['wn_top1_hypernyms']):\n",
    "        for j, hypernym in enumerate(hs):\n",
    "            hypernyms[word].append((syn_priority_l2, hypernym))\n",
    "            features[word].setdefault(hypernym, {})['syn1_priority_l2'] = 1\n",
    "            features[word].setdefault(hypernym, {})['syn1_priority_l2_pos'] = j\n",
    "            for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                features[word].setdefault(hyphyp, {})['syn1_priority_l3'] = 1\n",
    "                features[word].setdefault(hyphyp, {})['syn1_priority_l3_pos'] = j\n",
    "                hypernyms[word].append((syn_priority_l3, hyphyp))\n",
    "\n",
    "    for word, words in zip(df['word'], df['wn_top10']):\n",
    "        for synset_id in nouns[words[0]]:\n",
    "            hypernyms[word].append((syn_priority_l1, synset_id))\n",
    "            features[word].setdefault(synset_id, {})['syn1_priority_l1'] = 1\n",
    "            features[word].setdefault(synset_id, {})['syn1_priority_l1_pos'] = 0\n",
    "        for i, word2 in enumerate(words[1:]):\n",
    "            for j, synset_id in enumerate(nouns[word2]):\n",
    "                hypernyms[word].append((syntail_priority_l1 + i*1e-3, synset_id))\n",
    "                features[word].setdefault(synset_id, {})['syn%d_priority_l1'%(i+2)] = 1\n",
    "                features[word].setdefault(synset_id, {})['syn%d_priority_l1_pos'%(i+2)] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "try:\n",
    "    wn.all_synsets\n",
    "except LookupError as e:\n",
    "    import nltk\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_trailing_dot(s):\n",
    "    if s.endswith('.'):\n",
    "        return s[:-1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4697"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru2en = {}\n",
    "with open('data/ru.txt', 'r', encoding='utf-8') as f_ru, open('data/en_ya.txt', 'r', encoding='utf-8') as f_en_y:\n",
    "    for i, r, ey in zip(range(100500), f_ru, f_en_y):\n",
    "        r = drop_trailing_dot(r.strip())\n",
    "        ey = drop_trailing_dot(ey.strip())\n",
    "        ru2en[r] = ey\n",
    "len(ru2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8481"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ru = {}\n",
    "with open('data/hyp_en.txt', 'r', encoding=\"utf-8\") as f_en, open('data/hyp_ru_ya.txt', 'r', encoding=\"utf-8\") as f_ru_y:\n",
    "    for i, e, ruy, in zip(range(100500), f_en, f_ru_y):\n",
    "        e = drop_trailing_dot(e.strip())\n",
    "        ruy = drop_trailing_dot(ruy.strip())\n",
    "        en2ru[e] = ruy\n",
    "len(en2ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6476190463854875\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "missing = set()\n",
    "\n",
    "hypernyms_en = {}\n",
    "hypernyms_en_txt = {}\n",
    "for df in [df_test, df_train]:\n",
    "    cnt = 0\n",
    "    for word in df[\"word\"]:\n",
    "        hypernyms_en[word] = set()\n",
    "        hypernyms_en_txt[word] = set()\n",
    "        lword = word.lower()\n",
    "        if lword in ru2en:\n",
    "            synsets = wn.synsets(ru2en[lword])\n",
    "            if synsets:\n",
    "                flag = False\n",
    "                for sense in synsets:\n",
    "                    for hyp in sense.hypernyms():\n",
    "                        for name in hyp.lemma_names():\n",
    "                            name = name.replace('_', ' ')\n",
    "                            if name in en2ru:\n",
    "                                if en2ru[name].lower() in nouns:\n",
    "                                    flag = True\n",
    "                                    hypernyms_en_txt[word].add(en2ru[name].lower())\n",
    "                                    for id_ in nouns[en2ru[name].lower()]:\n",
    "                                        hypernyms[word].append((0.0, id_))\n",
    "                                        hypernyms_en[word].add((0.0, id_))\n",
    "                            else:\n",
    "                                missing.add(name)\n",
    "            if hypernyms_en[word]:\n",
    "                cnt += 1\n",
    "    print(cnt / (len(df[\"word\"])+1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "АБСОЛЮТИЗИРОВАТЬ set() set()\n",
      "АКТИВИРОВАТЬ {'модифицировать', 'изменять', 'менять', 'инициировать'} {(0.0, '106631-V'), (0.0, '111281-V'), (0.0, '116304-V'), (0.0, '124237-V'), (0.0, '106682-V'), (0.0, '117315-V'), (0.0, '106844-V')}\n",
      "АДСОРБИРОВАТЬ {'принять'} {(0.0, '124852-V'), (0.0, '115487-V'), (0.0, '106884-V'), (0.0, '146793-V'), (0.0, '145869-V'), (0.0, '134035-V'), (0.0, '141708-V')}\n",
      "АКАТЬ set() set()\n"
     ]
    }
   ],
   "source": [
    "for word in public_test[:2] + private_test[:2]:\n",
    "    print(word, hypernyms_en_txt[word], hypernyms_en[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in hypernyms_en:\n",
    "    for score, hypernym in hypernyms_en[word]:\n",
    "        features[word].setdefault(hypernym, {})['wordnet_en_l1'] = 1\n",
    "        for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "            features[word].setdefault(hyphyp, {})['wordnet_en_l2'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_words = {}\n",
    "def normalize(s):\n",
    "    res = []\n",
    "    for word in word_tokenize(s.lower()):\n",
    "        if word in norm_words:\n",
    "            res.append(norm_words[word])\n",
    "        else:\n",
    "            mp = morph.parse(word)\n",
    "            if mp:\n",
    "                norm_words[word] = mp[0].normal_form\n",
    "                res.append(norm_words[word])\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innertext(tag):\n",
    "    return (tag.text or '') + ''.join(innertext(e) for e in tag) + (tag.tail or '')\n",
    "\n",
    "def get_serp_texts(xml_path, k=5):\n",
    "    res = []\n",
    "    if os.path.exists(xml_path):\n",
    "        root = ET.parse(xml_path).getroot()\n",
    "        for e in root.find('response').find('results').find('grouping').findall('group')[:k]:\n",
    "            res.append(innertext(e.find('doc').find('title')))\n",
    "            if e.find('doc').find('passages'):\n",
    "                for passage in e.find('doc').find('passages'):\n",
    "                    res.append(innertext(passage))\n",
    "            if e.find('doc').find('headline'):\n",
    "                res.append(innertext(e.find('doc').find('headline')))\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 525/525 [00:30<00:00, 17.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 525/525 [00:52<00:00, 10.05it/s]\n"
     ]
    }
   ],
   "source": [
    "synset_norm_serp_ya_cnt = Counter()\n",
    "synset_norm_serp_g_cnt = Counter()\n",
    "hypernyms_wserp = {}\n",
    "serp_priority = -4.\n",
    "serp_hyp_priority = -4.\n",
    "meaning_priority = -1.\n",
    "\n",
    "for df in [\n",
    "    df_test,\n",
    "    df_train\n",
    "]:\n",
    "    for word in tqdm(df[\"word\"]):\n",
    "        word_file_path = 'data/google_it_all/' + word.lower() + '.tsv'\n",
    "        total_g_serp = \"\"\n",
    "        if os.path.exists(word_file_path):\n",
    "            with open(word_file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    text = line.split('\\t')[1]\n",
    "                    total_g_serp += text + \" \"\n",
    "        norm_total_g_serp = normalize(total_g_serp)\n",
    "        word_file_path = 'data/yandex_it_all/' + word.upper() + '.xml'\n",
    "        total_ya_serp = get_serp_texts(word_file_path, k=10)\n",
    "        norm_total_ya_serp = normalize(total_ya_serp)\n",
    "\n",
    "        total_meaning = \"\"\n",
    "        if word.lower() in ltitle2doc:\n",
    "            for meaning in parse_wiktionary(ltitle2doc[word.lower()]['text'])['meaning']:\n",
    "                total_meaning += meaning + \" \"\n",
    "        norm_total_meaning = normalize(total_meaning)\n",
    "\n",
    "        res = []\n",
    "        for score, hypernym in hypernyms[word]:\n",
    "            hypernym_texts = synset2words[hypernym] + [ruwordnet.get_name_by_id(hypernym)]\n",
    "            for hypernym_text in hypernym_texts:\n",
    "                norm_hypernym_text = normalize(hypernym_text)\n",
    "                if norm_hypernym_text in norm_total_g_serp:\n",
    "                    score += serp_priority\n",
    "                    features[word][hypernym]['serp_g_norm'] = 1\n",
    "                    for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                        if hyphyp in features[word]:\n",
    "                            features[word][hyphyp]['serp_g_norm_l2'] = 1\n",
    "                    synset_norm_serp_g_cnt[hypernym] += 1\n",
    "                if hypernym_text in total_g_serp:\n",
    "                    features[word][hypernym]['serp_g'] = 1\n",
    "\n",
    "                if norm_hypernym_text in norm_total_ya_serp:\n",
    "                    features[word][hypernym]['serp_ya_norm'] = 1\n",
    "                    synset_norm_serp_ya_cnt[hypernym] += 1\n",
    "                    for hyphyp in ruwordnet.get_hypernyms_by_id(hypernym):\n",
    "                        if hyphyp in features[word]:\n",
    "                            features[word][hyphyp]['serp_ya_norm_l2'] = 1\n",
    "                if hypernym_text in total_ya_serp:\n",
    "                    features[word][hypernym]['serp_ya'] = 1\n",
    "\n",
    "                if norm_hypernym_text in norm_total_meaning:\n",
    "                    score += meaning_priority\n",
    "                    features[word][hypernym]['meaning_norm'] = 1\n",
    "                if hypernym_text in total_meaning:\n",
    "                    features[word][hypernym]['meaning'] = 1\n",
    "            res.append((score, hypernym))\n",
    "        hypernyms_wserp[word] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = set()\n",
    "# for word in features:\n",
    "#     for synset_id in features[word]:\n",
    "#         for key in features[word][synset_id]:\n",
    "#             feature_names.add(key)\n",
    "# feature_names = sorted(feature_names)\n",
    "# len(feature_names)\n",
    "feature_names = [\n",
    "    \"meaning\", \n",
    "    \"meaning_norm\", \n",
    "#     \"serp_g\", \n",
    "#     \"serp_g_norm\", \n",
    "#     \"serp_g_norm_l2\", \n",
    "#     \"serp_ya\", \n",
    "#     \"serp_ya_norm\", \n",
    "#     \"serp_ya_norm_l2\", \n",
    "    \"syn1_priority_l1\", \n",
    "    \"syn1_priority_l2\", \n",
    "    \"syn1_priority_l3\", \n",
    "    \"syn2_priority_l1\", \n",
    "    \"syn2_priority_l2\", \n",
    "    \"syn2_priority_l3\", \n",
    "    \"syn3_priority_l1\", \n",
    "    \"syn3_priority_l2\", \n",
    "    \"syn3_priority_l3\", \n",
    "    \"syn4_priority_l1\", \n",
    "    \"syn4_priority_l2\", \n",
    "    \"syn4_priority_l3\", \n",
    "    \"syn5_priority_l1\", \n",
    "    \"syn5_priority_l2\", \n",
    "    \"syn5_priority_l3\", \n",
    "    \"syn6_priority_l1\", \n",
    "    \"syn6_priority_l2\", \n",
    "    \"syn6_priority_l3\", \n",
    "    \"syn7_priority_l1\", \n",
    "    \"syn7_priority_l2\", \n",
    "    \"syn7_priority_l3\", \n",
    "    \"wikhyp_priority_l1\", \n",
    "    \"wikhyp_priority_l2\", \n",
    "    \"wordnet_en_l1\", \n",
    "    \"wordnet_en_l2\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20640, 27), (20640,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = sum([len(features[x]) for x in df_train[\"word\"]])\n",
    "X = np.zeros( (total, len(feature_names)) )\n",
    "y = np.zeros( total )\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = 0\n",
    "for word in df_train[\"word\"]:\n",
    "    lword = word.lower()\n",
    "    true_hypernyms = set()\n",
    "    for synset_id in nouns[lword]:\n",
    "        true_hypernyms.update(ruwordnet.get_hypernyms_by_id(synset_id))\n",
    "    for synset_id in features[word]:\n",
    "        y[pos] = 1 if synset_id in true_hypernyms else 0\n",
    "        X[pos] = [features[word][synset_id].get(fn, 0) for fn in feature_names]\n",
    "        pos += 1\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16512,), (4128,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.9487468445052916\n",
      "0.003 0.9508602226071673\n",
      "0.01 0.952534185460138\n",
      "0.03 0.9496675240744257\n",
      "0.1 0.949092099343717\n",
      "0.3 0.9485525452455719\n",
      "1.0 0.9476796931865228\n",
      "3 0.9474360718070278\n",
      "10 0.9474196311004363\n",
      "30 0.9474136526616758\n",
      "100 0.9474002011744642\n",
      "300 0.9473987065647742\n",
      "1000 0.9474002011744643\n"
     ]
    }
   ],
   "source": [
    "for C in [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3, 10, 30, 100, 300, 1000]:\n",
    "    model = LogisticRegression(C=C)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(C, roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.03, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.03)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {k:v for k,v in zip(feature_names, model.coef_[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = {}\n",
    "for i in range(1, 11):\n",
    "    for j in range(1,4):\n",
    "        pos_weights['syn%d_priority_l%d_pos' % (i, j)] = -0.0001\n",
    "\n",
    "def calc_score(d):\n",
    "    score = 0.\n",
    "    for feature, weight in weights.items():\n",
    "        score += weight * d.get(feature, 0)\n",
    "    for feature, weight in pos_weights.items():\n",
    "        score += weight * d.get(feature, 0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'subm/subm106_no_gya'\n",
    "save_to_file({\n",
    "    k: get_top_hypernyms( [(-calc_score(features[k][x]), x) for x in features[k]] )\n",
    "    for k in public_test\n",
    "}, prefix + '_public_verbs.tsv', ruwordnet)\n",
    "save_to_file({\n",
    "    k: get_top_hypernyms( [(-calc_score(features[k][x]), x) for x in features[k]] )\n",
    "    for k in private_test\n",
    "}, prefix + '_private_verbs.tsv', ruwordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"meaning\": 0.15,\n",
    "    \"meaning_norm\": 0.15,\n",
    "    \"serp_g\": 0.20,\n",
    "    \"serp_g_norm\": 0.50,\n",
    "    \"serp_g_norm_l2\": 0.20,\n",
    "    \"serp_ya\": 0.15,\n",
    "    \"serp_ya_norm\": 0.20,\n",
    "    \"serp_ya_norm_l2\": 0.20,\n",
    "    \n",
    "    \"syn1_priority_l1\": 0.05,\n",
    "    \"syn1_priority_l2\": 0.50,\n",
    "    \"syn1_priority_l3\": 0.30,\n",
    "    \n",
    "    \"syn2_priority_l1\": 0.05,\n",
    "    \"syn2_priority_l2\": 0.25,\n",
    "    \"syn2_priority_l3\": 0.15,\n",
    "    \n",
    "    \"syn3_priority_l1\": 0.05,\n",
    "    \"syn3_priority_l2\": 0.25,\n",
    "    \"syn3_priority_l3\": 0.15,\n",
    "    \n",
    "    \"syn4_priority_l1\": 0.00,\n",
    "    \"syn4_priority_l2\": 0.10,\n",
    "    \"syn4_priority_l3\": 0.15,\n",
    "    \n",
    "    \"syn5_priority_l1\": 0.00,\n",
    "    \"syn5_priority_l2\": 0.10,\n",
    "    \"syn5_priority_l3\": 0.00,\n",
    "    \n",
    "    \"syn6_priority_l1\": 0.00,\n",
    "    \"syn6_priority_l2\": 0.10,\n",
    "    \"syn6_priority_l3\": 0.00,\n",
    "    \n",
    "    \"syn7_priority_l1\": 0.00,\n",
    "    \"syn7_priority_l2\": 0.10,\n",
    "    \"syn7_priority_l3\": 0.00,\n",
    "    \n",
    "    \"wikhyp_priority_l1\": 0.20,\n",
    "    \"wikhyp_priority_l2\": 0.50,\n",
    "    \n",
    "    \"wordnet_en_l1\": 0.05,\n",
    "    \"wordnet_en_l2\": 0.20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'subm/subm107_no_gya'\n",
    "save_to_file({\n",
    "    k: get_top_hypernyms( [(-calc_score(features[k][x]), x) for x in features[k]] )\n",
    "    for k in public_test\n",
    "}, prefix + '_public_verbs.tsv', ruwordnet)\n",
    "save_to_file({\n",
    "    k: get_top_hypernyms( [(-calc_score(features[k][x]), x) for x in features[k]] )\n",
    "    for k in private_test\n",
    "}, prefix + '_private_verbs.tsv', ruwordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
